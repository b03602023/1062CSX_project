#'
#' @param object ELM network object, produced using \code{\link{elm}}.
#' @param h Forecast horizon. If NULL then h is set to match frequency of time series.
#' @param y Optionally forecast using different data than what the network was trained on. Expected to create havoc and do really bad things!
#' @param xreg Exogenous regressors. Each column is a different regressor and the sample size must be at least as long as the target in-sample set plus the forecast horizon, but can be longer. Set it to NULL if no xreg inputs are used.
#' @param ... Unused argument.
#'
#' @return An object of classes "\code{forecast.net}" and "\code{forecast}".
#'   The function \code{plot} produces a plot of the forecasts.
#'   An object of class \code{"forecast.net"} is a list containing the following elements:
#' \itemize{
#' \item{\code{method}{ - The name of the forecasting method as a character string}}
#' \item{\code{mean}{ - Point forecasts as a time series}}
#' \item{\code{all.mean}{ - An array h x reps of all ensemble members forecasts, where reps are the number of ensemble members.}}
#' \item{\code{x}{ - The original time series used to create the network.}}
#' \item{\code{fitted}{ - Fitted values.}}
#' \item{\code{residuals}{ - Residuals from the fitted network.}}
#' }
#'
#' @author Nikolaos Kourentzes, \email{nikolaos@kourentzes.com}
#' @seealso \code{\link{elm}}, \code{\link{elm.thief}}, \code{\link{mlp}}.
#' @keywords elm thief ts
#'
#' @examples
#' \dontshow{
#'  fit <- elm(AirPassengers,reps=1)
#'  frc <- forecast(fit,h=36)
#'  print(frc)
#'  plot(frc)
#' }
#' \dontrun{
#'  fit <- elm(AirPassengers)
#'  plot(fit)
#'  frc <- forecast(fit,h=36)
#'  plot(frc)
#' }
#'
#' @export
#' @method forecast elm
forecast.elm <- function(object,h=NULL,y=NULL,xreg=NULL,...){
forecast.net(object,h=h,y=y,xreg=xreg,...)
}
predict <- forecast.elm(model)
forecast.net <- function(object,h=NULL,y=NULL,xreg=NULL,...){
# Produce forecast with NNs
if (is.null(y)){
y <- object$y
}
# Get frequency
ff.ls <- get.ff(y)
ff <- ff.ls$ff
ff.n <- ff.ls$ff.n
rm("ff.ls")
if (is.null(h)){
h <- max(ff)
}
# Get stuff from object list
cl.object <- class(object)
is.elm.fast <- any(cl.object == "elm.fast")
# Get information when neuralnets is used
if (!is.elm.fast){
net <- object$net
reps <- length(net$weights)
}
# Get additional ELM definitions
if (any(class(object) == "elm")){
direct <- object$direct
W.in <- object$W.in
W <- object$W
b <- object$b
W.dct <- object$W.dct
reps <- length(b)
} else {
direct <- FALSE
}
# Remaining common definitions
hd <- object$hd
lags <- object$lags
xreg.lags <- object$xreg.lags
difforder <- object$difforder
sdummy <- object$sdummy
det.type <- object$det.type
minmax <- object$minmax
xreg.minmax <- object$xreg.minmax
comb <- object$comb
fitted <- object$fitted
ff.det <- object$ff.det
ff.n.det <- length(ff.det)
# Temporal aggregation can mess-up start/end of ts, so lets fix it
fstart <- c(end(y)[1],end(y)[2]+1)
if (is.na(fstart[2])){  # If the second element of end(y) does not exist because it is fractional
fstart <- end(y) + deltat(y)
}
# Check xreg inputs
if (!is.null(xreg)){
x.n <- dim(xreg)[2]
if (length(xreg.lags) != x.n){
stop("Number of xreg inputs is not consistent with network specification (number of xreg.lags).")
}
if (dim(xreg)[1] < length(y)+h){
stop("Length of xreg must be longer that y + forecast horizon.")
}
} else {
x.n <- 0
}
# Apply differencing
d <- length(difforder)
y.d <- y.ud <- vector("list",d+1)
y.d[[1]] <- y
names(y.d)[1] <- "d0"
if (d>0){
for (i in 1:d){
y.d[[i+1]] <- diff(y.d[[i]],difforder[i])
names(y.d)[i+1] <- paste0(names(y.d)[i],"d",difforder[i])
}
}
Y <- as.vector(linscale(y.d[[d+1]],minmax=minmax)$x)
# Scale xreg
if (x.n > 0){
xreg.sc <- xreg
for (i in 1:x.n){
xreg.sc[,i] <- linscale(xreg[,i],minmax=xreg.minmax[[i]])$x
}
# Starting point of xreg
xstart <- length(y)+1
}
if (sdummy == TRUE){
temp <- ts(1:h,start=fstart,frequency=max(ff.det))
Xd <- vector("list",ff.n.det)
for (s in 1:ff.n.det){
Xd[[s]] <- seasdummy(h,m=ff.det[s],y=temp,type=det.type)
colnames(Xd[[s]]) <- paste0("D",s,".",1:dim(Xd[[s]])[2])
if (det.type=="trg"){
Xd[[s]] <- Xd[[s]][,1:2]
}
}
Xd <- do.call(cbind,Xd)
# Xd <- seasdummy(h,y=temp,type=det.type)
}
Yfrc <- array(NA,c(h,reps),dimnames=list(paste0("t+",1:h),paste0("NN.",1:reps)))
if (length(lags)>0){
ylag <- max(lags)
} else {
ylag <- 0
}
# For each repetition
for (r in 1:reps){
frc.sc <- vector("numeric",h)
for (i in 1:h){
# Construct inputs
if (i == 1){
temp <- NULL
} else {
temp <- frc.sc[1:(i-1)]
}
xi <- rev(tail(c(Y,temp),ylag)) # Reverse for lags
xi <- xi[lags]
# Construct xreg inputs
if (x.n > 0){
Xreg <- vector("list",x.n)
for (j in 1:x.n){
if (length(xreg.lags[[j]])>0){
xreg.temp <- xreg.sc[(xstart+i-1):(xstart-max(xreg.lags[[j]])+i-1),j] # Reversing is happening in the indices
Xreg[[j]] <- xreg.temp[xreg.lags[[j]]+1]
}
}
Xreg.all <- unlist(Xreg)
xi <- c(xi,Xreg.all)
}
xi <- rbind(xi)
# Construct seasonal dummies inputs
if (sdummy == TRUE){
xi <- cbind(xi,Xd[i,,drop=FALSE])
}
# Calculate forecasts
if (any(class(object) == "mlp")){
yhat.sc <- neuralnet::compute(net,xi,r)$net.result
} else {
# EML
if (is.elm.fast){
yhat.sc <- predict.elm.fast.internal(xi,W.in[[r]],W[[r]],b[r],W.dct[[r]],direct)
} else {
H <- t(as.matrix(tail(neuralnet::compute(net,xi,r)$neurons,1)[[1]][,2:(tail(hd,1)+1)]))
yhat.sc <- H %*% W[[r]] + b[r] + if(direct!=TRUE){0}else{xi %*% W.dct[[r]]}
}
}
frc.sc[i] <- yhat.sc
}
# Reverse scaling
frc <- linscale(frc.sc,minmax,rev=TRUE)$x
# Reverse differencing
f.ud <- vector("list",d+1)
names(f.ud) <- names(y.ud)
f.ud[[d+1]] <- frc
if (d>0){
for (i in 1:d){
temp <- c(tail(y.d[[d+1-i]],difforder[d+1-i]),f.ud[[d+2-i]])
n.t <- length(temp)
for (j in 1:(n.t-difforder[d+1-i])){
temp[difforder[d+1-i]+j] <- temp[j] + temp[difforder[d+1-i]+j]
}
f.ud[[d+1-i]] <- temp[(difforder[d+1-i]+1):n.t]
}
}
fout <- head(f.ud,1)[[1]]
Yfrc[,r] <- fout
}
# Combine forecasts
fout <- frc.comb(Yfrc,comb)
fout <- ts(fout,frequency=frequency(y),start=fstart)
# Prepare output
out <- list("method"=class(object),"mean"=fout,
"all.mean"=ts(Yfrc,frequency=frequency(y),start=fstart),
"x"=y,"fitted"=fitted,"residuals"=y-fitted)
return(structure(out,class=c("forecast.net","forecast")))
}
(model)
predict <- forecast.elm(model)
?get.ff
install.packages("ff")
library(ff)
predict <- forecast.elm(model)
x = ts(tp_data[,2])
model <- elm(x)
predict <- forecast.elm(model)
predict
predict <- forecast(model)
#--------ML 3-------------
#nnfor package
library(nnfor)
x = ts(tp_data[,1])
model <- elm(x)
predict <- forecast(model)
forecast
?forecast
m <- mlp(x)
pred <- forecast(m)
View(pred)
p <- predict(mlp)
?predict
### Crawler_Example with rvest    #####################################################################
# 參考：https://blog.gtwang.org/r/rvest-web-scraping-with-r/
rm(list = ls())
library(rvest)
# Set url
url <- "https://www.ptt.cc/bbs/TW_Entertain/index"
index.no <- 500:518
paste0(url, index, ".html")
paste0(url, index.no, ".html")
# Set url
prefix <- "https://www.ptt.cc/bbs/TW_Entertain/index"
index.no <- 500:518
url <- paste0(url, index.no, ".html")
paste0(url, index.no, ".html")
url <- paste0(prefix, index.no, ".html")
url
# Get response
res <- read_html(url)
# Get response
res <- read_html(url[1])
# Get response
res <- read_html(url[1])
res
setwd("D:/D disk/unicourse/106shia/github/1062CSX_project/final project")
getwd()
url <- "https://zh.wikipedia.org/zh-tw/%E7%B6%9C%E8%97%9D%E5%A4%A7%E7%86%B1%E9%96%80"
# Get response
res <- read_html(url)
# Parse the content and extract the titles
# . for class; # for ID
raw.titles <- res %>% html_nodes("table#wikitable")
# Parse the content and extract the titles
# . for class; # for ID
raw.titles <- html_nodes(res,"table.wikitable")
library(rvest)
library(rvest)
url <- "https://zh.wikipedia.org/zh-tw/%E7%B6%9C%E8%97%9D%E5%A4%A7%E7%86%B1%E9%96%80"
# Get response
res <- read_html(url)
# Parse the content and extract the titles
# . for class; # for ID
raw.titles <- html_nodes(res,"table.wikitable")
library(rvest)
url <- "https://zh.wikipedia.org/zh-tw/%E7%B6%9C%E8%97%9D%E5%A4%A7%E7%86%B1%E9%96%80"
# Get response
res <- read_html(url)
# Parse the content and extract the titles
# . for class; # for ID
raw.titles <- html_nodes(res,"table.wikitable")
text <- html_text(html_nodes(x =res, css = "table.wikitable tbody tr td"))
html_text(raw.titles )
text <- html_text(raw.titles)
text
length(text)
class(text)
View(text)
text <- text[[26:27]]
text <- text[26:27]
View(text)
t1 <- text[[1]]
t1
t1[1]
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1
library(rvest)
url <- "https://zh.wikipedia.org/zh-tw/%E7%B6%9C%E8%97%9D%E5%A4%A7%E7%86%B1%E9%96%80"
# Get response
res <- read_html(url)
# Parse the content and extract the titles
# . for class; # for ID
raw.titles <- html_nodes(res,"table.wikitable")
text <- html_text(html_nodes(x =res, css = "table.wikitable tbody tr td"))
text <- html_text(raw.titles)
View(text)
text <- text[22:24]
t1 <- text[[1]]
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1
l <- length(t1)
l
13%/%4
13%%4
l%%4
t1[l]
t1[l][1]
?gsub
split?
d
?split
split(t1[l],4)
split(t1[l],sep="年")
strsplit(t1[l],sep="年")
strsplit(t1[l],spilt="年")
strsplit(t1[l],split="年")
strsplit(t1[l],split="年")[[1]]==2017
strsplit(t1[l],split="年")[[1]][1]==2017
sortout <- function(t, yr){
for(i in 1:length(t)){
if(strsplit(t[i],split="年")[[1]][1]==yr){
temp <- t[-i]
}
}
return(temp)
}
t1 <- sortout(t1,2017)
t1 <- sortout(t1,"2017")
length(t1)
length(t1)-5
t1[1:7]
length(t1)-5
(length(t1)-5)/6
(length(t1)-5)/21
?seq
l <- (length(t1)-5)/21
l
seq(21, (length(t)-5), by=21)
seq(21, (length(t1)-5), by=21)
sortout <- function(t){
p <- seq(21, (length(t)-5), by=21)
t <- t[-p]
return(t)
}
t1 <- sortout(t1)
t1
t1 <- t1[-(seq(21, (length(t)-5), by=21))]
t1 <- t1[-(seq(21, (length(t1)-5), by=21))]
t1
t1 <- t1[-(1:5)]
t1 <- text[[1]]
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1 <- t1[-(1:5)]
t1
t1[1]
sortout <- function(t){
p <- seq(21, length(t), by=21)
t <- t[-p]
return(t)
}
t1 <- sortout(t1)
t1
length(t1)
sortout <- function(t){
p <- seq(21, length(t), by=21)
t <- t[-p]
p <- seq(5, length(t), by=5)
t <- t[-p]
return(t)
}
t1 <- sortout(t1)
t1 <- text[[1]]
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1 <- t1[-(1:5)]
sortout <- function(t){
p <- seq(21, length(t), by=21)
t <- t[-p]
p <- seq(5, length(t), by=5)
t <- t[-p]
return(t)
}
t1 <- sortout(t1)
t1
t1 <- text[[1]]
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1 <- t1[-(1:5)]
t1
p <- seq(21, length(t1), by=21)
t1 <- t1[-p]
t1
p <- seq(5, length(t), by=5)
p <- seq(5, length(t1), by=5)
p1
p
t <- t[-p]
t1 <- t1[-p]
t1
t1 <- text[[1]]
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1 <- t1[-(1:5)]
p <- seq(21, length(t1), by=21)
t1 <- t1[-p]
p <- seq(5, length(t1), by=5)
t1
t1 <- text[[1]]
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1 <- t1[-(1:5)]
sortout <- function(t){
p <- seq(21, length(t), by=21)
t <- t[-p]
p <- numeric()
for(i in 1:length(t)){
if(t[i]==""){p=c(p,i)}
}
t <- t[-p]
return(t)
}
t1 <- sortout(t1)
t1
t1 <- text[[1]]
t1
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1 <- t1[-(1:5)]
t1
13%%4
c <-c(2,1,3,5)
cc <-seq(1,14,1)
cc[-c]
cc
sortout <- function(t){
p <- numeric()
q <- numeric()
for(i in 1:length(t)){
if(t[i]==""){p=c(p,i)}
if(length(p)%%4==0){q=c(q,length(p)+1)}
}
t <- t[-c(p,q)]
return(t)
}
t1 <- sortout(t1)
t1 <- text[[1]]
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1 <- t1[-(1:5)]
sortout <- function(t){
p <- numeric()
q <- numeric()
for(i in 1:length(t)){
if(t[i]==""){p=c(p,i)}
if(length(p)%%4==0){q=c(q,length(p)+1)}
}
t <- t[-c(p,q)]
return(t)
}
t1 <- sortout(t1)
t1
text <- text[22:24]
t1 <- text[[1]]
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1 <- t1[-(1:5)]
t1
text <- html_text(html_nodes(x =res, css = "table.wikitable tbody tr td"))
text <- html_text(raw.titles)
text <- text[22:24]
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1 <- text[[1]]
t1 <- t1[-(1:5)]
t1
# Parse the content and extract the titles
# . for class; # for ID
raw.titles <- html_nodes(res,"table.wikitable")
text <- html_text(html_nodes(x =res, css = "table.wikitable tbody tr td"))
text <- html_text(raw.titles)
text <- text[22:24]
t1 <- text[[1]]
t1 <- unlist(strsplit(t1,split="\n", fixed=T))
t1 <- t1[-(1:5)]
t1
sortout <- function(t){
p <- numeric()
q <- numeric()
d1 <- 0     #d1和d2的作用就是避免嘉賓有空格而占兩個位置情況產生的bug
d2 <- 1
for(i in 1:length(t)){
if(t[i]==""){
p=c(p,i)
if(length(p)%%4==0){d1=d1+1}
}
if(d1==d2){q=c(q,i+1);d2=d2+1}
}
t <- t[-c(p,q)]
return(t)
}
t1 <- sortout(t1)
t1
