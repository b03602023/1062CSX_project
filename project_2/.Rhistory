require("Rfacebook")
fbOAuth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
fbOAuth
#---------Rfacebook--------
token = "EAACEdEose0cBAPWqw5JDTSaYbWgWYcCsGJ11ZAuZBbFR6ZAZBlahtxdas6Sk2ERLKRBW1V3rZCJNjXo1Fp26ABZA85gyiLgi85fDhUt4CbwTIZBcLTUmsZCZCJNUwKvpCW2o5m2DxSWvby8TS5v0ETvk2srZB7UvU43TvcVPlR6pcz4CaFMvLYQxnZB92efbGzrZAX8ZD"
library(Rfacebook)
me <- getUsers("me", token, private_info = TRUE)
me$name
require("Rfacebook")
fbOAuth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
me <- getUsers("me",token=fb.oauth)
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
me <- getUsers("me",token=fb.oauth)
me$name
token = "EAACEdEose0cBAPWqw5JDTSaYbWgWYcCsGJ11ZAuZBbFR6ZAZBlahtxdas6Sk2ERLKRBW1V3rZCJNjXo1Fp26ABZA85gyiLgi85fDhUt4CbwTIZBcLTUmsZCZCJNUwKvpCW2o5m2DxSWvby8TS5v0ETvk2srZB7UvU43TvcVPlR6pcz4CaFMvLYQxnZB92efbGzrZAX8ZD"
library(Rfacebook)
me <- getUsers("me", token, private_info = TRUE)
me$name
install.packages("httpuv")
install.packages("httpuv")
library(httpuv)
require("Rfacebook")
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE,
type = "application/x-www-form-urlencoded"
)
require("Rfacebook")
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
wordcloud(g_freqFrame$Var1,g_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:20)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
library(Matrix)   #nnzero
library(wordcloud2)
tres <- 10
g_freqFrame = as.data.frame(table(unlist(g_seg)))
library(httr)
pageno <- 1:10    #?œå??æ•¸
g_url <- paste0("https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E5%A5%B3%E8%A3%9D&page=", pageno, "&sort=rnk/dc")
b_url <- paste0("https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E7%94%B7%E8%A3%9D&page=", pageno, "&sort=rnk/dc")
#url = "https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E5%A5%B3&page=1&sort=rnk/dc"
g_prods = character()
b_prods = character()
getcontent <- function(url, prods){
for(i in 1:length(pageno)){
res = GET(url[i])
res_json = httr::content(res)
results <- data.frame(do.call(rbind,res_json$prods))
prods[((pageno[i]-1)*20+1):(pageno[i]*20)] <- unlist(results$name)
}
return(prods)
}
g_prods <- c(g_prods, getcontent(g_url, g_prods))
Sys.sleep(60)
b_prods <- c(b_prods, getcontent(b_url, b_prods))
library(Matrix)   #nnzero
library(wordcloud2)
tres <- 10
g_freqFrame = as.data.frame(table(unlist(g_seg)))
#-------clean data--------
processdata <- function(prods){
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)      #?·è???library(RColorBrewer)
library(wordcloud)
library(tmcn)   #segmentCN
docs <- Corpus(VectorSource(prods))
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
# æ¸…é™¤å¤§å?å¯«è‹±?‡è??¸å?
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
#ç§»é™¤æ¨™é?ç¬¦è? (punctuation)
#ç§»é™¤?¸å? (digits)?ç©º??(white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#-------?·è?--------
mixseg = worker()
#new_user_word(mixseg,segment)   #Add user word
#?·è?  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
#1
seg = lapply(docs, jieba_tokenizer)
#2
seg = lapply(docs, segmentCN)
#seg = lapply(seg, strsplit, " ")
return(seg)
}
g_seg <- processdata(g_prods)
b_seg <- processdata(b_prods)
library(Matrix)   #nnzero
library(wordcloud2)
tres <- 10
g_freqFrame = as.data.frame(table(unlist(g_seg)))
# å°‡å¥³è£é??µå??»é™¤
wordcloud2(g_freqFrame[(g_freqFrame$Freq>tres)&(g_freqFrame$Freq!=max(g_freqFrame$Freq)),], fontFamily = "å¾®è??…é?",color = "random-light", backgroundColor = "grey", size = 1
)
windowsFonts(TC=windowsFont("Heiti TC Light"))
wordcloud(g_freqFrame$Var1,g_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:20)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(g_freqFrame$Var1,g_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(b_freqFrame$Var1,b_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
b_freqFrame = as.data.frame(table(unlist(b_seg)))
tres <-10
windowsFonts(TC=windowsFont("Heiti TC Light"))
wordcloud(b_freqFrame$Var1,b_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
library(rvest)
page <- 0:12
p <- 48*(page)
u_getrvest <- function(url){
res <- read_html(url)
raw.titles <- res %>% html_nodes("div.unit .info .name")
u_prods <- raw.titles %>% html_node("a") %>% html_attr('title')
return(u_prods)
}
ug_url <- paste0("http://www.uniqlo.com/tw/store/search?qtext=%E5%A5%B3%E8%A3%9D&qbrand=10&qclv1=&qclv25=&qclv2=&qrange=&qcolor=&qsize=&qnew=&qdiscount=&qlimit=&qmulti=&qonline=&qspsize=&qstart=", p,"&sort=goods_disp_priority")
ug_prods <- mapply(u_getrvest, ug_url)
colnames(ug_prods) <- page+1
#raw.prices <- res %>% html_nodes("dd.price") %>% html_text
#raw.prices <- as.character(raw.prices)
#gsub("NT$", "", raw.prices)
ug_seg <- processdata(as.character(ug_prods))
ug_freqFrame = as.data.frame(table(unlist(ug_seg)))
tres <-10
windowsFonts(TC=windowsFont("Heiti TC Light"))
wordcloud(ug_freqFrame$Var1,ug_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:17)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ug_freqFrame$Var1,ug_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
ub_url <- paste0("http://www.uniqlo.com/tw/store/search?qtext=%E7%94%B7%E8%A3%9D&qbrand=10&qclv1=&qclv25=&qclv2=&qrange=&qcolor=&qsize=&qnew=&qdiscount=&qlimit=&qmulti=&qonline=&qspsize=&qstart=", p,"&sort=goods_disp_priority")
ub_prods <- mapply(u_getrvest, ub_url)
colnames(ub_prods) <- page+1
#raw.prices <- res %>% html_nodes("dd.price") %>% html_text
#raw.prices <- as.character(raw.prices)
#gsub("NT$", "", raw.prices)
ub_seg <- processdata(as.character(ub_prods))
ub_freqFrame = as.data.frame(table(unlist(ub_seg)))
tres <-10
windowsFonts(TC=windowsFont("Heiti TC Light"))
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
ub_freqFrame
?wordcloud
wordcloud(g_freqFrame$Var1,g_freqFrame$Freq,
scale=c(10,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(g_freqFrame$Var1,g_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(g_freqFrame$Var1,g_freqFrame$Freq,
scale=c(10,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
#-------è©žé »-------
freqFramecbind(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame)
#-------è©žé »-------
freqFrame <- cbind(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame)
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(10,5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(10,0.1),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(10,0.9),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(10,1),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(20,5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(20,3),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(15,1),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(10,1),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
#-------è©žé »-------
freqFrame <- merge(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame)
#-------è©žé »-------
freqFrame <- merge(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame, all.x = TRUE)
ub_freqFrame
#-------è©žé »-------
freqFrame <- merge(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame,
by = "Var1")
?merge
#-------è©žé »-------
g_seg <- Corpus(VectorSource(g_seg))
g_tdm <- TermDocumentMatrix(g_seg, control = list(wordLengths = c(1,10)))
g_tf <- as.matrix(g_tdm)/apply(g_tdm, 2, sum)   #term frequency: the number of words in every document
g_idf <- log10(ncol(g_tdm)/apply(g_tdm, 1, nnzero))
g_tfidf <- g_tf*g_idf       #TF-IDF
g_tfidf
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)]))       #TF-IDF
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE),]))       #TF-IDF
head(g_tfidf[order(g_tfidf, decreasing = TRUE),])
g_tfidf[order(g_tfidf, decreasing = TRUE),]
g_tfidf
g_tfidf <- apply(g_tfidf, 1, sum)
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE),]))       #TF-IDF
g_tfidf
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)]))       #TF-IDF
View(g_tfidf[order(g_tfidf, decreasing = TRUE)])       #TF-IDF
?head
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
g_tf <- as.matrix(g_tdm)/apply(g_tdm, 2, sum)   #term frequency: the number of words in every document
g_idf <- log10(ncol(g_tdm)/apply(g_tdm, 1, nnzero))
g_tfidf <- g_tf*g_idf       #TF-IDF
g_tfidf <- apply(g_tfidf, 1, sum)
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
b_tf <- as.matrix(b_tdm)/apply(b_tdm, 2, sum)   #term frequency: the number of words in every document
b_idf <- log10(ncol(b_tdm)/apply(b_tdm, 1, nnzero))
b_tfidf <- b_tf*b_idf       #TF-IDF
b_tfidf <- apply(b_tfidf, 1, sum)
View(head(b_tfidf[order(b_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ug_tf <- as.matrix(ug_tdm)/apply(ug_tdm, 2, sum)   #term frequency: the number of words in every document
ug_idf <- log10(ncol(ug_tdm)/apply(ug_tdm, 1, nnzero))
ug_tfidf <- ug_tf*ug_idf       #TF-IDF
ug_tfidf <- apply(ug_tfidf, 1, sum)
View(head(ug_tfidf[order(ug_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ub_tf <- as.matrix(ub_tdm)/apply(ub_tdm, 2, sum)   #term frequency: the number of words in every document
ub_idf <- log10(ncol(ub_tdm)/apply(ub_tdm, 1, nnzero))
ub_tfidf <- ub_tf*ub_idf       #TF-IDF
ub_tfidf <- apply(ub_tfidf, 1, sum)
View(head(ub_tfidf[order(ub_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
b_tf <- as.matrix(b_tdm)/apply(b_tdm, 2, sum)   #term frequency: the number of words in every document
b_seg <- Corpus(VectorSource(b_seg))
b_tdm <- TermDocumentMatrix(b_seg, control = list(wordLengths = c(1,10)))
b_tf <- as.matrix(b_tdm)/apply(b_tdm, 2, sum)   #term frequency: the number of words in every document
b_idf <- log10(ncol(b_tdm)/apply(b_tdm, 1, nnzero))
b_tfidf <- b_tf*b_idf       #TF-IDF
b_tfidf <- apply(b_tfidf, 1, sum)
View(head(b_tfidf[order(b_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ug_seg <- Corpus(VectorSource(ug_seg))
ug_tdm <- TermDocumentMatrix(ug_seg, control = list(wordLengths = c(1,10)))
ug_tf <- as.matrix(ug_tdm)/apply(ug_tdm, 2, sum)   #term frequency: the number of words in every document
ug_idf <- log10(ncol(ug_tdm)/apply(ug_tdm, 1, nnzero))
ug_tfidf <- ug_tf*ug_idf       #TF-IDF
ug_tfidf <- apply(ug_tfidf, 1, sum)
View(head(ug_tfidf[order(ug_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ub_seg <- Corpus(VectorSource(ub_seg))
ub_tdm <- TermDocumentMatrix(ub_seg, control = list(wordLengths = c(1,10)))
ub_tf <- as.matrix(ub_tdm)/apply(ub_tdm, 2, sum)   #term frequency: the number of words in every document
ub_idf <- log10(ncol(ub_tdm)/apply(ub_tdm, 1, nnzero))
ub_tfidf <- ub_tf*ub_idf       #TF-IDF
ub_tfidf <- apply(ub_tfidf, 1, sum)
View(head(ub_tfidf[order(ub_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
library(httr)
pageno <- 1:10    #?œå??æ•¸
g_url <- paste0("https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E5%A5%B3%E8%A3%9D&page=", pageno, "&sort=rnk/dc")
b_url <- paste0("https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E7%94%B7%E8%A3%9D&page=", pageno, "&sort=rnk/dc")
#url = "https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E5%A5%B3&page=1&sort=rnk/dc"
g_prods = character()
b_prods = character()
getcontent <- function(url, prods){
for(i in 1:length(pageno)){
res = GET(url[i])
res_json = httr::content(res)
results <- data.frame(do.call(rbind,res_json$prods))
prods[((pageno[i]-1)*20+1):(pageno[i]*20)] <- unlist(results$name)
}
return(prods)
}
g_prods <- c(g_prods, getcontent(g_url, g_prods))
Sys.sleep(60)
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
# Import library
library(e1071)
# Importing the dataset
data(iris)
# Create x and y
x <- subset(iris, select = -Species)
q()
setwd("D:/D disk/unicourse/106shia/github/1062CSX_project/project_2")
library(xgboost)
library(Matrix)
set.seed(1234)
train <- read.csv("train.csv")
test  <- read.csv("test.csv")
##### Removing IDs
train$ID <- NULL
test.id <- test$ID
test$ID <- NULL
### Storing the columns' minima and maxima
maxmin <- data.frame()
i <- 1
for(c in (names(train))){
maxmin[i,1] <- min(train[train$TARGET==1,c])
maxmin[i,2] <- max(train[train$TARGET==1,c])
i=i+1
}
row.names(maxmin) <- names(train)
names(maxmin) <- c('min', 'max')
##### Extracting TARGET  ???Æ±?TARGET?o???Ü¼Æ¨????U?????Þ§@?v?T
#target is the outcome of our dataset meaning it is the binary classification we will try to predict.
train.y <- train$TARGET
train$TARGET <- NULL
##### 0 count per line
count0 <- function(x) {
return( sum(x == 0) )
}
train$n0 <- apply(train, 1, FUN=count0)
test$n0 <- apply(test, 1, FUN=count0)
##### Removing constant features
cat("\n## Removing the constants features.\n")
for (f in names(train)) {
if (length(unique(train[[f]])) == 1) {   #with duplicate elements/rows removed
cat(f, "is constant in train. We delete it.\n")
train[[f]] <- NULL
test[[f]] <- NULL
}
}
##### Removing identical features
features_pair <- combn(names(train), 2, simplify = F) #?Õ¦X(2?Ó¬??@??)
toRemove <- c()
for(pair in features_pair) {
f1 <- pair[1]
f2 <- pair[2]
if (!(f1 %in% toRemove) & !(f2 %in% toRemove)) {
if (all(train[[f1]] == train[[f2]])) {
cat(f1, "and", f2, "are equals.\n")
toRemove <- c(toRemove, f2)
}
}
}
library(dplyr)
feature.names <- setdiff(names(train), toRemove)  #???X?S?Ê¤??P???Ü¼?(???S?Ê¬Û¦P???Ü¼Æ²???)
train$var38 <- log(train$var38)
test$var38 <- log(test$var38)
maxmin['var38', 'min'] <- log(maxmin['var38', 'min'])
maxmin['var38', 'max'] <- log(maxmin['var38', 'max'])
train <- train[, feature.names]
test <- test[, feature.names]
#---limit vars in test based on min and max vals of train
# ??train?Ì¤p?È³Ì¤j?È¨Ó¬É©wtest???Ì¤p?È³Ì¤j??
print('Setting min-max lims on test data')
for(f in colnames(train)){
lim <- min(train[,f])
test[test[,f]<lim,f] <- lim
lim <- max(train[,f])
test[test[,f]>lim,f] <- lim
}
#---
train$TARGET <- train.y
train <- sparse.model.matrix(TARGET ~ ., data = train)
#advance feature-- xgb.DMatrix
dtrain <- xgb.DMatrix(data=train, label=train.y)
watchlist <- list(train=dtrain)
param <- list(  objective           = "binary:logistic",
booster             = "gbtree",
eval_metric         = "auc",
eta                 = 0.0202048,
max_depth           = 5,
subsample           = 0.6815,
colsample_bytree    = 0.701
)
clf <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = 560,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE
)    #advanced interface for training an xgboost model
xgb.plot.tree(clf)
xgb.plot.tree(clf)
bst <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = 5,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE
)    #advanced interface for training an xgboost model
xgb.plot.tree(bst)
class(bst)
class(model = bst)
xgb.plot.tree( model = bst)
library(xgb.plot.tree)
install.packages(xgb.plot.tree)
install.packages("xgb.plot.tree")
library(xgb.plot.tree)
install.packages("xgb.plot.tree")
test$TARGET <- -1
test_cp <- test
test_cp$TARGET <- NULL
test_cp$n0 <- NULL
test <- sparse.model.matrix(TARGET ~ ., data = test)
#perform the prediction
#These numbers doesn??t look like binary classification {0,1}. We need
#to perform a simple transformation before being able to use these results.
preds <- predict(clf, test)
head(preds)
plot(preds)
?class.ind
??class.ind
