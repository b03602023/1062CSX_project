docs <- tm_map(docs,toSpace, "1")
# æ¸…é™¤å¤§å?å¯«è‹±?‡è??¸å?
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
#ç§»é™¤æ¨™é?ç¬¦è? (punctuation)
#ç§»é™¤?¸å? (digits)?ç©º??(white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#-------?·è?--------
mixseg = worker()
#new_user_word(mixseg,segment)   #Add user word
#?·è?  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
#1
seg = lapply(docs, jieba_tokenizer)
#2
seg = lapply(docs, segmentCN)
#seg = lapply(seg, strsplit, " ")
return(seg)
}
g_seg <- processdata(g_prods)
b_seg <- processdata(b_prods)
library(Matrix)   #nnzero
library(wordcloud2)
tres <- 10
g_freqFrame = as.data.frame(table(unlist(g_seg)))
# å°‡å¥³è£é??µå??»é™¤
wordcloud2(g_freqFrame[(g_freqFrame$Freq>tres)&(g_freqFrame$Freq!=max(g_freqFrame$Freq)),], fontFamily = "å¾®è??…é?",color = "random-light", backgroundColor = "grey", size = 1
)
windowsFonts(TC=windowsFont("Heiti TC Light"))
wordcloud(g_freqFrame$Var1,g_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:20)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(g_freqFrame$Var1,g_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(b_freqFrame$Var1,b_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
b_freqFrame = as.data.frame(table(unlist(b_seg)))
tres <-10
windowsFonts(TC=windowsFont("Heiti TC Light"))
wordcloud(b_freqFrame$Var1,b_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
library(rvest)
page <- 0:12
p <- 48*(page)
u_getrvest <- function(url){
res <- read_html(url)
raw.titles <- res %>% html_nodes("div.unit .info .name")
u_prods <- raw.titles %>% html_node("a") %>% html_attr('title')
return(u_prods)
}
ug_url <- paste0("http://www.uniqlo.com/tw/store/search?qtext=%E5%A5%B3%E8%A3%9D&qbrand=10&qclv1=&qclv25=&qclv2=&qrange=&qcolor=&qsize=&qnew=&qdiscount=&qlimit=&qmulti=&qonline=&qspsize=&qstart=", p,"&sort=goods_disp_priority")
ug_prods <- mapply(u_getrvest, ug_url)
colnames(ug_prods) <- page+1
#raw.prices <- res %>% html_nodes("dd.price") %>% html_text
#raw.prices <- as.character(raw.prices)
#gsub("NT$", "", raw.prices)
ug_seg <- processdata(as.character(ug_prods))
ug_freqFrame = as.data.frame(table(unlist(ug_seg)))
tres <-10
windowsFonts(TC=windowsFont("Heiti TC Light"))
wordcloud(ug_freqFrame$Var1,ug_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:17)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ug_freqFrame$Var1,ug_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
ub_url <- paste0("http://www.uniqlo.com/tw/store/search?qtext=%E7%94%B7%E8%A3%9D&qbrand=10&qclv1=&qclv25=&qclv2=&qrange=&qcolor=&qsize=&qnew=&qdiscount=&qlimit=&qmulti=&qonline=&qspsize=&qstart=", p,"&sort=goods_disp_priority")
ub_prods <- mapply(u_getrvest, ub_url)
colnames(ub_prods) <- page+1
#raw.prices <- res %>% html_nodes("dd.price") %>% html_text
#raw.prices <- as.character(raw.prices)
#gsub("NT$", "", raw.prices)
ub_seg <- processdata(as.character(ub_prods))
ub_freqFrame = as.data.frame(table(unlist(ub_seg)))
tres <-10
windowsFonts(TC=windowsFont("Heiti TC Light"))
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
ub_freqFrame
?wordcloud
wordcloud(g_freqFrame$Var1,g_freqFrame$Freq,
scale=c(10,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(g_freqFrame$Var1,g_freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(g_freqFrame$Var1,g_freqFrame$Freq,
scale=c(10,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
#-------è©é »-------
freqFramecbind(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame)
#-------è©é »-------
freqFrame <- cbind(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame)
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(10,5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(10,0.1),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(10,0.9),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(10,1),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(20,5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(20,3),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(15,1),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(10,1),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
#-------è©é »-------
freqFrame <- merge(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame)
#-------è©é »-------
freqFrame <- merge(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame, all.x = TRUE)
ub_freqFrame
#-------è©é »-------
freqFrame <- merge(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame,
by = "Var1")
?merge
#-------è©é »-------
g_seg <- Corpus(VectorSource(g_seg))
g_tdm <- TermDocumentMatrix(g_seg, control = list(wordLengths = c(1,10)))
g_tf <- as.matrix(g_tdm)/apply(g_tdm, 2, sum)   #term frequency: the number of words in every document
g_idf <- log10(ncol(g_tdm)/apply(g_tdm, 1, nnzero))
g_tfidf <- g_tf*g_idf       #TF-IDF
g_tfidf
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)]))       #TF-IDF
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE),]))       #TF-IDF
head(g_tfidf[order(g_tfidf, decreasing = TRUE),])
g_tfidf[order(g_tfidf, decreasing = TRUE),]
g_tfidf
g_tfidf <- apply(g_tfidf, 1, sum)
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE),]))       #TF-IDF
g_tfidf
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)]))       #TF-IDF
View(g_tfidf[order(g_tfidf, decreasing = TRUE)])       #TF-IDF
?head
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
g_tf <- as.matrix(g_tdm)/apply(g_tdm, 2, sum)   #term frequency: the number of words in every document
g_idf <- log10(ncol(g_tdm)/apply(g_tdm, 1, nnzero))
g_tfidf <- g_tf*g_idf       #TF-IDF
g_tfidf <- apply(g_tfidf, 1, sum)
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
b_tf <- as.matrix(b_tdm)/apply(b_tdm, 2, sum)   #term frequency: the number of words in every document
b_idf <- log10(ncol(b_tdm)/apply(b_tdm, 1, nnzero))
b_tfidf <- b_tf*b_idf       #TF-IDF
b_tfidf <- apply(b_tfidf, 1, sum)
View(head(b_tfidf[order(b_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ug_tf <- as.matrix(ug_tdm)/apply(ug_tdm, 2, sum)   #term frequency: the number of words in every document
ug_idf <- log10(ncol(ug_tdm)/apply(ug_tdm, 1, nnzero))
ug_tfidf <- ug_tf*ug_idf       #TF-IDF
ug_tfidf <- apply(ug_tfidf, 1, sum)
View(head(ug_tfidf[order(ug_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ub_tf <- as.matrix(ub_tdm)/apply(ub_tdm, 2, sum)   #term frequency: the number of words in every document
ub_idf <- log10(ncol(ub_tdm)/apply(ub_tdm, 1, nnzero))
ub_tfidf <- ub_tf*ub_idf       #TF-IDF
ub_tfidf <- apply(ub_tfidf, 1, sum)
View(head(ub_tfidf[order(ub_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
b_tf <- as.matrix(b_tdm)/apply(b_tdm, 2, sum)   #term frequency: the number of words in every document
b_seg <- Corpus(VectorSource(b_seg))
b_tdm <- TermDocumentMatrix(b_seg, control = list(wordLengths = c(1,10)))
b_tf <- as.matrix(b_tdm)/apply(b_tdm, 2, sum)   #term frequency: the number of words in every document
b_idf <- log10(ncol(b_tdm)/apply(b_tdm, 1, nnzero))
b_tfidf <- b_tf*b_idf       #TF-IDF
b_tfidf <- apply(b_tfidf, 1, sum)
View(head(b_tfidf[order(b_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ug_seg <- Corpus(VectorSource(ug_seg))
ug_tdm <- TermDocumentMatrix(ug_seg, control = list(wordLengths = c(1,10)))
ug_tf <- as.matrix(ug_tdm)/apply(ug_tdm, 2, sum)   #term frequency: the number of words in every document
ug_idf <- log10(ncol(ug_tdm)/apply(ug_tdm, 1, nnzero))
ug_tfidf <- ug_tf*ug_idf       #TF-IDF
ug_tfidf <- apply(ug_tfidf, 1, sum)
View(head(ug_tfidf[order(ug_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ub_seg <- Corpus(VectorSource(ub_seg))
ub_tdm <- TermDocumentMatrix(ub_seg, control = list(wordLengths = c(1,10)))
ub_tf <- as.matrix(ub_tdm)/apply(ub_tdm, 2, sum)   #term frequency: the number of words in every document
ub_idf <- log10(ncol(ub_tdm)/apply(ub_tdm, 1, nnzero))
ub_tfidf <- ub_tf*ub_idf       #TF-IDF
ub_tfidf <- apply(ub_tfidf, 1, sum)
View(head(ub_tfidf[order(ub_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
library(httr)
pageno <- 1:10    #?œå??æ•¸
g_url <- paste0("https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E5%A5%B3%E8%A3%9D&page=", pageno, "&sort=rnk/dc")
b_url <- paste0("https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E7%94%B7%E8%A3%9D&page=", pageno, "&sort=rnk/dc")
#url = "https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E5%A5%B3&page=1&sort=rnk/dc"
g_prods = character()
b_prods = character()
getcontent <- function(url, prods){
for(i in 1:length(pageno)){
res = GET(url[i])
res_json = httr::content(res)
results <- data.frame(do.call(rbind,res_json$prods))
prods[((pageno[i]-1)*20+1):(pageno[i]*20)] <- unlist(results$name)
}
return(prods)
}
g_prods <- c(g_prods, getcontent(g_url, g_prods))
Sys.sleep(60)
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
# Import library
library(e1071)
# Importing the dataset
data(iris)
# Create x and y
x <- subset(iris, select = -Species)
q()
getwd()
setwd("D:/D disk/unicourse/106shia/github/1062CSX_project/week_12")
mnist <- dataset_mnist()
library(keras)
mnist <- dataset_mnist()
mnist <- dataset_mnist()
mnist <- dataset_mnist()
library(tibble)
library(readr)
install.packages("tibble")
install.pachkages("readr")
install.packages("readr")
library(tibble)
library(readr)
read.csv("jena_climate_2009_2016.csv",header = TRUE)
data <- read.csv("jena_climate_2009_2016.csv",header = TRUE)
str(data)
glimpse(data)
?str
library(ggplot2)
install.packages("ggplot2")
str(dat)
str(data)
data$T (degC)
View(data)
data$T..degC.
ggplot(data, aes(x = 1:nrow(data), y = `T (degC)`)) + geom_line()
library(ggplot2)
ggplot(data, aes(x = 1:nrow(data), y = `T (degC)`)) + geom_line()
ggplot(data, aes(x = 1:nrow(data), y = `T..degC.`)) + geom_line()
ggplot(data[1:1440,], aes(x = 1:1440, y = `T (degC)`)) + geom_line()
ggplot(data[1:1440,], aes(x = 1:1440, y = `T..degC.`)) + geom_line()
sequence_generator <- function(start) {
value <- start - 1
function() {
value <<- value + 1
value
}
}
gen <- sequence_generator(10)
gen()
gen()
gen()
class(gen)
mode(gen)
data <- data.matrix(data[,-1])
ncol(data)
?apply(array, margin, ...)
#Use the first 200000 timesptes as training data.
train_data <- data[1:200000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data <- scale(data, center = mean, scale = std)
?scale
apply(data, 2, mean)
apply(data, 2, sd)
#The data is recorded every 10 minutes.
#Thus, 144 data points are collected in each day.
data <- read.csv("jena_climate_2009_2016.csv",header = TRUE)
apply(scale(data, center = apply(data, 2, mean), scale = apply(data, 2, sd)), 2, mean)
#Discard the first column because the it is text type.
data <- data.matrix(data[,-1])
apply(scale(data, center = apply(data, 2, mean), scale = apply(data, 2, sd)), 2, mean)
apply(scale(data, center = apply(data, 2, mean), scale = apply(data, 2, sd)), 2, sd)
#Step 1, normalize the data.
#Use the first 200000 timesptes as training data for the normalization of all data points.
train_data <- data[1:200000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
#Use scale() function and the training data to normalize the whole data
#æª¢æ¸¬scaleçš„åŠŸèƒ½->å°‡è³‡æ–™çš„å¹³å‡è®Šæˆ0ã€æ¨™æº–å·®è®Šæˆ1ã€‚
#apply(scale(data, center = apply(data, 2, mean), scale = apply(data, 2, sd)), 2, mean)
#apply(scale(data, center = apply(data, 2, mean), scale = apply(data, 2, sd)), 2, sd)
data <- scale(data, center = mean, scale = std)
apply(data, 2, mean)
apply(data, 2, sd)
nrow(data)
rows = c(1:13)
rows
c(length(rows)))
c(length(rows))
targets <- array(0, dim = c(length(rows)))
dim(targets)
targets <- array(0, dim = c(length(rows),1,3))
targets
dim(targets)
dim(targets)[2]
dim(targets)[3]
dim(targets)[[1]]
# Time Series Forecasting with Recurrent Neural Networks
# https://tensorflow.rstudio.com/blog/time-series-forecasting-with-recurrent-neural-networks.html
library(tibble)
library(readr)
library(ggplot2)
#The data is recorded every 10 minutes.
#Thus, 144 data points are collected in each day.
data <- read.csv("jena_climate_2009_2016.csv",header = TRUE)
str(data)
#year scale
ggplot(data, aes(x = 1:nrow(data), y = `T..degC.`)) + geom_line()
# a scale of days (10 days)
ggplot(data[1:1440,], aes(x = 1:1440, y = `T..degC.`)) + geom_line()
#If you were trying to predict average temperature for the next month given a few months of
#past data, the problem would be easy, due to the reliable year-scale periodicity of the data.
#But looking at the data over a scale of days, the temperature looks a lot more chaotic.
#Is this time series predictable at a daily scale? Letâ€™s find out.
#Discard the first column because the it is text type.
data <- data.matrix(data[,-1])
#Step 1, normalize the data.
#Use the first 200000 timesptes as training data for the normalization of all data points.
train_data <- data[1:200000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
#Use scale() function and the training data to normalize the whole data
#æª¢æ¸¬scaleçš„åŠŸèƒ½->å°‡è³‡æ–™çš„å¹³å‡è®Šæˆ0ã€æ¨™æº–å·®è®Šæˆ1ã€‚
#apply(scale(data, center = apply(data, 2, mean), scale = apply(data, 2, sd)), 2, mean)
#apply(scale(data, center = apply(data, 2, mean), scale = apply(data, 2, sd)), 2, sd)
data <- scale(data, center = mean, scale = std)
generator <- function(data, lookback, delay, min_index, max_index,
shuffle = FALSE, batch_size = 128, step = 6) {
if (is.null(max_index))
max_index <- nrow(data) - delay - 1
i <- min_index + lookback
function() {
if (shuffle) {
rows <- sample(c((min_index+lookback):max_index), size = batch_size)
} else {
if (i + batch_size >= max_index)
i <<- min_index + lookback
rows <- c(i:min(i+batch_size-1, max_index))
i <<- i + length(rows)
}
samples <- array(0, dim = c(length(rows),
lookback / step,
dim(data)[[-1]]))
targets <- array(0, dim = c(length(rows)))
#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
for (j in 1:length(rows)) {
indices <- seq(rows[[j]] - lookback, rows[[j]]-1,
length.out = dim(samples)[[2]])
samples[j,,] <- data[indices,]
targets[[j]] <- data[rows[[j]] + delay,2]
}
list(samples, targets)
}
}
lookback <- 1440
step <- 6
delay <- 144
batch_size <- 128
train_gen <- generator(
data,
lookback = lookback,
delay = delay,
min_index = 1,
max_index = 200000,
shuffle = TRUE,
step = step,
batch_size = batch_size
)
val_gen = generator(
data,
lookback = lookback,
delay = delay,
min_index = 200001,
max_index = 300000,
step = step,
batch_size = batch_size
)
test_gen <- generator(
data,
lookback = lookback,
delay = delay,
min_index = 300001,
max_index = NULL,
step = step,
batch_size = batch_size
)
library(keras)
evaluate_naive_method <- function() {
batch_maes <- c()
for (step in 1:val_steps) {
c(samples, targets) %<-% val_gen()
preds <- samples[,dim(samples)[[2]],2]
mae <- mean(abs(preds - targets))
batch_maes <- c(batch_maes, mae)
}
print(mean(batch_maes))
}
evaluate_naive_method()
install.packages("keras")
library(keras)
evaluate_naive_method <- function() {
batch_maes <- c()
for (step in 1:val_steps) {
c(samples, targets) %<-% val_gen()
preds <- samples[,dim(samples)[[2]],2]
mae <- mean(abs(preds - targets))
batch_maes <- c(batch_maes, mae)
}
print(mean(batch_maes))
}
evaluate_naive_method()
# How many steps to draw from val_gen in order to see the entire validation set
val_steps <- (300000 - 200001 - lookback) / batch_size
# How many steps to draw from test_gen in order to see the entire test set
test_steps <- (nrow(data) - 300001 - lookback) / batch_size
evaluate_naive_method <- function() {
batch_maes <- c()
for (step in 1:val_steps) {
c(samples, targets) %<-% val_gen()
preds <- samples[,dim(samples)[[2]],2]
mae <- mean(abs(preds - targets))
batch_maes <- c(batch_maes, mae)
}
print(mean(batch_maes))
}
evaluate_naive_method()
std[[2]]
0.29 * std[[2]]
