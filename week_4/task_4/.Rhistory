idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
tfidf <- tf*idf       #TF-IDF
# View(head(tfidf))       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
s_tfidf <- s_tfidf[order(s_tfidf, decreasing = TRUE)]
#s_tfidf[s_tfidf>0.23]
tfidf <- as.data.frame(tfidf)
s_tfidf[s_tfidf>0.23][1:10]
setwd("D:/D disk/unicourse/106shia/github/1062CSX_project/week_5/task_5")
#-------clean data--------
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)      #斷詞用
library(RColorBrewer)
library(wordcloud)
library(tmcn)   #segmentCN
filenames <- list.files(getwd(), pattern="*.txt")  #pattern: an optional regular expression. Only file names which match the regular expression will be returned.
files <- lapply(filenames, readLines)  #Read some or all text lines from a connection.
docs <- Corpus(VectorSource(files))  #Representing and computing on corpora(語料庫).
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
# 清除大小寫英文與數字
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#-------斷詞--------
mixseg = worker()
segment <- c("新北")
new_user_word(mixseg,segment)   #Add user word
#斷詞  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
#轉成文件
#詞頻結果:
library(Matrix)   #nnzero
freqFrame = as.data.frame(table(unlist(seg)))
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
# 可用matrix呈現: as.matrix(tdm)
#-------caculate  TF-IDF ----------
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
tfidf <- tf*idf       #TF-IDF
# View(head(tfidf))       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
s_tfidf <- s_tfidf[order(s_tfidf, decreasing = TRUE)]
#s_tfidf[s_tfidf>0.23]
tfidf <- as.data.frame(tfidf)
s_tfidf[s_tfidf>0.23][1:10]
setwd("D:/D disk/unicourse/106shia/github/1062CSX_project/week_4/task_4")
# week_5 task_5 TF-IDF for week_4 低碳生活部落格
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)      #斷詞用
library(RColorBrewer)
library(wordcloud)
library(tmcn)   #segmentCN
filenames <- list.files(getwd(), pattern="*.txt")  #pattern: an optional regular expression. Only file names which match the regular expression will be returned.
files <- lapply(filenames, readLines)  #Read some or all text lines from a connection.
docs <- Corpus(VectorSource(files))  #Representing and computing on corpora(語料庫).
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}     # Create content transformers, i.e., functions which modify the content of an R objec
# gsub performs replacement of all matches.(以空白取代)
#定義清洗：清洗就是把你找到的符號用空白取代
)
# content_transformer 為內文取代 function
# tm_map(x, FUN, ...)
# Interface to apply transformation functions (also denoted as mappings) to corpora.
# x: A corpus. 語料庫("Corpus" is a collection of text documents.)
# FUN: a transformation function taking a text document (a character vector when x is a SimpleCorpus) as input and returning a text document (a character vector of the same length as the input vector for SimpleCorpus). The function content_transformer can be used to create a wrapper to get and set the content of text documents.
# 複製文本中你不要的符號刪除，可寫多行刪除多種符號
#docs <- tm_map(docs, toSpace, "文本中你不要的符號貼於此")
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
docs <- tm_map(docs,toSpace, "的")
docs <- tm_map(docs,toSpace, "及")
docs <- tm_map(docs,toSpace, "為")
docs <- tm_map(docs,toSpace, "是")
docs <- tm_map(docs,toSpace, "在")
docs <- tm_map(docs,toSpace, "我")
docs <- tm_map(docs,toSpace, "沒有")
docs <- tm_map(docs,toSpace, "不要")
docs <- tm_map(docs,toSpace, "只")
docs <- tm_map(docs,toSpace, "正要")
docs <- tm_map(docs,toSpace, "尤其")
docs <- tm_map(docs,toSpace, "做")
docs <- tm_map(docs,toSpace, "活動")
docs <- tm_map(docs,toSpace, "停止")
docs <- tm_map(docs,toSpace, "一下")
docs <- tm_map(docs,toSpace, "進行")
docs <- tm_map(docs,toSpace, "要")
docs <- tm_map(docs,toSpace, "們")
docs <- tm_map(docs,toSpace, "有")
docs <- tm_map(docs,toSpace, "要")
docs <- tm_map(docs,toSpace, "就")
docs <- tm_map(docs,toSpace, "也")
docs <- tm_map(docs,toSpace, "會")
docs <- tm_map(docs,toSpace, "台灣")
docs <- tm_map(docs,toSpace, "台北")
docs <- tm_map(docs,toSpace, "就")
docs <- tm_map(docs,toSpace, "生活")
docs <- tm_map(docs,toSpace, "更")
docs <- tm_map(docs,toSpace, "不")
docs <- tm_map(docs,toSpace, "讓")
docs <- tm_map(docs,toSpace, "都")
docs <- tm_map(docs,toSpace, "與")
docs <- tm_map(docs,toSpace, "小編")
docs <- tm_map(docs,toSpace, "就")
docs <- tm_map(docs,toSpace, "喔")
docs <- tm_map(docs,toSpace, "新")
docs <- tm_map(docs,toSpace, "年")
docs <- tm_map(docs,toSpace, "可以")
docs <- tm_map(docs,toSpace, "啊")
docs <- tm_map(docs,toSpace, "文章")
docs <- tm_map(docs,toSpace, "到")
docs <- tm_map(docs,toSpace, "將")
docs <- tm_map(docs,toSpace, "演講")
docs <- tm_map(docs,toSpace, "格")
docs <- tm_map(docs,toSpace, "一個")
docs <- tm_map(docs,toSpace, "以")
docs <- tm_map(docs,toSpace, "中")
docs <- tm_map(docs,toSpace, "下午")
docs <- tm_map(docs,toSpace, "但")
docs <- tm_map(docs,toSpace, "吧")
docs <- tm_map(docs,toSpace, "還")
docs <- tm_map(docs,toSpace, "如何")
docs <- tm_map(docs,toSpace, "將")
docs <- tm_map(docs,toSpace, "﹍")
# 清除大小寫英文與數字
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
# 語詞詞幹化(stemmization)
# 以英文為例
#https://zh.wikipedia.org/wiki/词干提取
#library(SnowballC)
#確保任何形式的單字只會轉換成相同詞性出現一次
#docs <- tm_map(docs, stemDocument)
##Stem words in a text document using Porter's stemming algorithm
#在 worker() 內可以設定各種不同的全切分法模型與引用外部詞庫，在這裡直接使用
#預設的全切分法的混合模型，與 jieba 自帶的詞庫。
# 一般斷詞
mixseg = worker()
#適時的增加詞庫
# segment <- c("陳菊","布里斯本","高雄","重劃區","合作會","後勁溪")
segment <- c("低碳生活部落格","綠建築","節能","氣候變遷","電動車","台達電子","台達電子文教基金會","綠色能源")
new_user_word(mixseg,segment)   #Add user word
#斷詞  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
#轉成文件
#詞頻結果:
library(Matrix)   #nnzero
freqFrame = as.data.frame(table(unlist(seg)))
#seg <- as.data.frame(unlist(seg))
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
# 可用matrix呈現: as.matrix(tdm)
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
tfidf <- tf*idf       #TF-IDF
View(tfidf)
View(tdm)
tdm[["dimnames"]]
?TermDocumentMatrix
seg <- tm_map(docs, stemDocument)
seg
View(seg)
View(seg)
seg[["1"]][["content"]]
class(tdm)
class(docs)
tdm <- tm_map(tdm, toSpace, "[A-Za-z0-9]")
inspect(tdm)
#------visualize----
library(ggplot2)
t <- 0.5
reorder_size <- function(x) { factor(x, levels = names(sort(table(x)))) }
names(s_tfidf) <- reorder_size(names(s_tfidf))
p <- ggplot(tfidf[s_tfidf>t,], aes(reorder_size(names(s_tfidf[s_tfidf>t])), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1))+
coord_flip()
s_tfidf <- apply(tfidf, 1, sum)
s_tfidf <- s_tfidf[order(s_tfidf, decreasing = TRUE)]
#s_tfidf[s_tfidf>0.23]
tfidf <- as.data.frame(tfidf)
#------visualize----
library(ggplot2)
t <- 0.5
reorder_size <- function(x) { factor(x, levels = names(sort(table(x)))) }
names(s_tfidf) <- reorder_size(names(s_tfidf))
p <- ggplot(tfidf[s_tfidf>t,], aes(reorder_size(names(s_tfidf[s_tfidf>t])), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1))+
coord_flip()
t <- 0.4
reorder_size <- function(x) { factor(x, levels = names(sort(table(x)))) }
names(s_tfidf) <- reorder_size(names(s_tfidf))
p <- ggplot(tfidf[s_tfidf>t,], aes(reorder_size(names(s_tfidf[s_tfidf>t])), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1))+
coord_flip()
t <- 0.28
reorder_size <- function(x) { factor(x, levels = names(sort(table(x)))) }
names(s_tfidf) <- reorder_size(names(s_tfidf))
p <- ggplot(tfidf[s_tfidf>t,], aes(reorder_size(names(s_tfidf[s_tfidf>t])), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1))+
coord_flip()
p <- ggplot(tfidf[s_tfidf>t,], aes(reorder_size(names(s_tfidf[s_tfidf>t])), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1))+
coord_flip()+
xlab("TF-IDF") + ylab("字詞")
p <- ggplot(tfidf[s_tfidf>t,], aes(reorder_size(names(s_tfidf[s_tfidf>t])), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
, size = 2)+
coord_flip()+
xlab("TF-IDF") + ylab("字詞")
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
, axis.title.x = element_text(size = 2))+
coord_flip()+
xlab("TF-IDF") + ylab("字詞")
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
)+
coord_flip()+
xlab("TF-IDF") + ylab("字詞")
?findFreqTerms
tm::findFreqTerms(tdm)
inspect(tm::findFreqTerms(tdm))
View(tm::findFreqTerms(tdm))
inspect(tdm)
View(tm::findFreqTerms(tdm))
View(tm::findFreqTerms(tdm, 0.5))
View(tm::findFreqTerms(tdm, 22))
?TermDocumentMatrix
View(files)
files <- readtext("*.txt", encoding = "big5")
??readtext
install.packages("readtext")
library(readtext)
files <- readtext("*.txt", encoding = "big5")
View(files)
docs <- Corpus(VectorSource(files))  #Representing and computing on corpora(語料庫).
View(docs)
# week_5 task_5 TF-IDF for week_4 低碳生活部落格
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)      #斷詞用
library(RColorBrewer)
library(wordcloud)
library(readtext)
library(tmcn)   #segmentCN
#filenames <- list.files(getwd(), pattern="*.txt")  #pattern: an optional regular expression. Only file names which match the regular expression will be returned.
#files <- lapply(filenames, readLines)  #Read some or all text lines from a connection.
files <- readtext("*.txt", encoding = "big5")
docs <- Corpus(VectorSource(files))  #Representing and computing on corpora(語料庫).
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}     # Create content transformers, i.e., functions which modify the content of an R objec
# gsub performs replacement of all matches.(以空白取代)
#定義清洗：清洗就是把你找到的符號用空白取代
)
# content_transformer 為內文取代 function
# tm_map(x, FUN, ...)
# Interface to apply transformation functions (also denoted as mappings) to corpora.
# x: A corpus. 語料庫("Corpus" is a collection of text documents.)
# FUN: a transformation function taking a text document (a character vector when x is a SimpleCorpus) as input and returning a text document (a character vector of the same length as the input vector for SimpleCorpus). The function content_transformer can be used to create a wrapper to get and set the content of text documents.
# 複製文本中你不要的符號刪除，可寫多行刪除多種符號
#docs <- tm_map(docs, toSpace, "文本中你不要的符號貼於此")
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
docs <- tm_map(docs,toSpace, "的")
docs <- tm_map(docs,toSpace, "及")
docs <- tm_map(docs,toSpace, "為")
docs <- tm_map(docs,toSpace, "是")
docs <- tm_map(docs,toSpace, "在")
docs <- tm_map(docs,toSpace, "我")
docs <- tm_map(docs,toSpace, "沒有")
docs <- tm_map(docs,toSpace, "不要")
docs <- tm_map(docs,toSpace, "只")
docs <- tm_map(docs,toSpace, "正要")
docs <- tm_map(docs,toSpace, "尤其")
docs <- tm_map(docs,toSpace, "做")
docs <- tm_map(docs,toSpace, "活動")
docs <- tm_map(docs,toSpace, "停止")
docs <- tm_map(docs,toSpace, "一下")
docs <- tm_map(docs,toSpace, "進行")
docs <- tm_map(docs,toSpace, "要")
docs <- tm_map(docs,toSpace, "們")
docs <- tm_map(docs,toSpace, "有")
docs <- tm_map(docs,toSpace, "要")
docs <- tm_map(docs,toSpace, "就")
docs <- tm_map(docs,toSpace, "也")
docs <- tm_map(docs,toSpace, "會")
docs <- tm_map(docs,toSpace, "台灣")
docs <- tm_map(docs,toSpace, "台北")
docs <- tm_map(docs,toSpace, "就")
docs <- tm_map(docs,toSpace, "生活")
docs <- tm_map(docs,toSpace, "更")
docs <- tm_map(docs,toSpace, "不")
docs <- tm_map(docs,toSpace, "讓")
docs <- tm_map(docs,toSpace, "都")
docs <- tm_map(docs,toSpace, "與")
docs <- tm_map(docs,toSpace, "小編")
docs <- tm_map(docs,toSpace, "就")
docs <- tm_map(docs,toSpace, "喔")
docs <- tm_map(docs,toSpace, "新")
docs <- tm_map(docs,toSpace, "年")
docs <- tm_map(docs,toSpace, "可以")
docs <- tm_map(docs,toSpace, "啊")
docs <- tm_map(docs,toSpace, "文章")
docs <- tm_map(docs,toSpace, "到")
docs <- tm_map(docs,toSpace, "將")
docs <- tm_map(docs,toSpace, "演講")
docs <- tm_map(docs,toSpace, "格")
docs <- tm_map(docs,toSpace, "一個")
docs <- tm_map(docs,toSpace, "以")
docs <- tm_map(docs,toSpace, "中")
docs <- tm_map(docs,toSpace, "下午")
docs <- tm_map(docs,toSpace, "但")
docs <- tm_map(docs,toSpace, "吧")
docs <- tm_map(docs,toSpace, "還")
docs <- tm_map(docs,toSpace, "如何")
docs <- tm_map(docs,toSpace, "將")
docs <- tm_map(docs,toSpace, "﹍")
# 清除大小寫英文與數字
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
# 語詞詞幹化(stemmization)
# 以英文為例
#https://zh.wikipedia.org/wiki/词干提取
#library(SnowballC)
#確保任何形式的單字只會轉換成相同詞性出現一次
#docs <- tm_map(docs, stemDocument)
##Stem words in a text document using Porter's stemming algorithm
#在 worker() 內可以設定各種不同的全切分法模型與引用外部詞庫，在這裡直接使用
#預設的全切分法的混合模型，與 jieba 自帶的詞庫。
# 一般斷詞
mixseg = worker()
#適時的增加詞庫
# segment <- c("陳菊","布里斯本","高雄","重劃區","合作會","後勁溪")
segment <- c("低碳生活部落格","綠建築","節能","氣候變遷","電動車","台達電子","台達電子文教基金會","綠色能源")
new_user_word(mixseg,segment)   #Add user word
#斷詞  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
#轉成文件
#詞頻結果:
library(Matrix)   #nnzero
#seg <- as.data.frame(unlist(seg))
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
# 可用matrix呈現: as.matrix(tdm)
#-------caculate  TF-IDF ----------
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
tfidf <- tf*idf       #TF-IDF
View(head(tfidf))
head(as.matrix(tdm))
View(files)
docs <- Corpus(VectorSource(files$text))  #Representing and computing on corpora(語料庫).
View(docs)
View(files)
View(docs)
length(files)
View(docs)
View(docs)
#filenames <- list.files(getwd(), pattern="*.txt")  #pattern: an optional regular expression. Only file names which match the regular expression will be returned.
#files <- lapply(filenames, readLines)  #Read some or all text lines from a connection.
files <- readtext("*.txt", encoding = "big5")
docs <- Corpus(VectorSource(files$text))  #Representing and computing on corpora(語料庫).
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}     # Create content transformers, i.e., functions which modify the content of an R objec
# gsub performs replacement of all matches.(以空白取代)
#定義清洗：清洗就是把你找到的符號用空白取代
)
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
# 一般斷詞
mixseg = worker()
#適時的增加詞庫
# segment <- c("陳菊","布里斯本","高雄","重劃區","合作會","後勁溪")
segment <- c("低碳生活部落格","綠建築","節能","氣候變遷","電動車","台達電子","台達電子文教基金會","綠色能源")
new_user_word(mixseg,segment)   #Add user word
#斷詞  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
#轉成文件
#詞頻結果:
library(Matrix)   #nnzero
freqFrame = as.data.frame(table(unlist(seg)))
#seg <- as.data.frame(unlist(seg))
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
# 可用matrix呈現: as.matrix(tdm)
#-------caculate  TF-IDF ----------
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
tfidf <- tf*idf       #TF-IDF
View(head(tfidf))       #TF-IDF
write.csv(tfidf, "show.csv")
View(tf)
s_tf <- apply(tf, 1, sum)
head(s_tf)
s_idf <- apply(idf, 1, sum)
View(cbind(s_tf, idf, s_tfidf))
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
s_tf <- apply(tf, 1, sum)
idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
tfidf <- tf*idf       #TF-IDF
#write.csv(tfidf, "show.csv")
# View(head(tfidf))       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
View(cbind(s_tf, idf, s_tfidf))
colnames(all) <- c("TF", "IDF", "TF-IDF")
names(all) <- c("TF", "IDF", "TF-IDF")
class(all)
all <- cbind(s_tf, idf, s_tfidf)
class(all)
mode(all)
colnames(all) <- c("TF", "IDF", "TF-IDF")
View(all)
View(tfidf)
p <- ggplot(tfidf[s_tfidf>t,], aes(reorder_size(names(s_tfidf[s_tfidf>t])), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
)+
coord_flip()+
xlab("TF-IDF") + ylab("字詞")
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
s_tf <- apply(tf, 1, sum)
idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
tfidf <- tf*idf       #TF-IDF
#write.csv(tfidf, "show.csv")
# View(head(tfidf))       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
all <- cbind(s_tf, idf, s_tfidf)
colnames(all) <- c("TF", "IDF", "TF-IDF")
s_tfidf <- s_tfidf[order(s_tfidf, decreasing = TRUE)]
#s_tfidf[s_tfidf>0.23]
tfidf <- as.data.frame(tfidf)
#------visualize----
library(ggplot2)
t <- 0.28
reorder_size <- function(x) { factor(x, levels = names(sort(table(x)))) }
names(s_tfidf) <- reorder_size(names(s_tfidf))
p <- ggplot(tfidf[s_tfidf>t,], aes(reorder_size(names(s_tfidf[s_tfidf>t])), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
)+
coord_flip()+
xlab("TF-IDF") + ylab("字詞")
??kable
library(knitr)
freqFrame = as.data.frame(table(unlist(seg)))
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
# 可用matrix呈現: as.matrix(tdm)
kable(inspect(tdm))
?head
kable(View(cbind(head(all), tail(all))))
kable(View(head(all)))
kable(View(cbind(all[1:10,], all[(nrow(all)-15):nrow(all)])))
kable(View(rbind(all[1:10,], all[(nrow(all)-15):nrow(all)])))
View(rbind(all[1:10,], all[(nrow(all)-15):nrow(all)]))
mode(all)
class(all)
kable(rbind(all[1:10,], all[(nrow(all)-15):nrow(all)]))
all[(nrow(all)-15):nrow(all)]
kable(rbind(all[1:10,], all[(nrow(all)-15):nrow(all),]))
