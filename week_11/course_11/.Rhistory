rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
wordcloud(ub_freqFrame$Var1,ub_freqFrame$Freq,
scale=c(10,1),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:25)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
#-------è©žé »-------
freqFrame <- merge(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame)
#-------è©žé »-------
freqFrame <- merge(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame, all.x = TRUE)
ub_freqFrame
#-------è©žé »-------
freqFrame <- merge(ub_freqFrame, ug_freqFrame, b_freqFrame, g_freqFrame,
by = "Var1")
?merge
#-------è©žé »-------
g_seg <- Corpus(VectorSource(g_seg))
g_tdm <- TermDocumentMatrix(g_seg, control = list(wordLengths = c(1,10)))
g_tf <- as.matrix(g_tdm)/apply(g_tdm, 2, sum)   #term frequency: the number of words in every document
g_idf <- log10(ncol(g_tdm)/apply(g_tdm, 1, nnzero))
g_tfidf <- g_tf*g_idf       #TF-IDF
g_tfidf
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)]))       #TF-IDF
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE),]))       #TF-IDF
head(g_tfidf[order(g_tfidf, decreasing = TRUE),])
g_tfidf[order(g_tfidf, decreasing = TRUE),]
g_tfidf
g_tfidf <- apply(g_tfidf, 1, sum)
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE),]))       #TF-IDF
g_tfidf
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)]))       #TF-IDF
View(g_tfidf[order(g_tfidf, decreasing = TRUE)])       #TF-IDF
?head
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
g_tf <- as.matrix(g_tdm)/apply(g_tdm, 2, sum)   #term frequency: the number of words in every document
g_idf <- log10(ncol(g_tdm)/apply(g_tdm, 1, nnzero))
g_tfidf <- g_tf*g_idf       #TF-IDF
g_tfidf <- apply(g_tfidf, 1, sum)
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
b_tf <- as.matrix(b_tdm)/apply(b_tdm, 2, sum)   #term frequency: the number of words in every document
b_idf <- log10(ncol(b_tdm)/apply(b_tdm, 1, nnzero))
b_tfidf <- b_tf*b_idf       #TF-IDF
b_tfidf <- apply(b_tfidf, 1, sum)
View(head(b_tfidf[order(b_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ug_tf <- as.matrix(ug_tdm)/apply(ug_tdm, 2, sum)   #term frequency: the number of words in every document
ug_idf <- log10(ncol(ug_tdm)/apply(ug_tdm, 1, nnzero))
ug_tfidf <- ug_tf*ug_idf       #TF-IDF
ug_tfidf <- apply(ug_tfidf, 1, sum)
View(head(ug_tfidf[order(ug_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ub_tf <- as.matrix(ub_tdm)/apply(ub_tdm, 2, sum)   #term frequency: the number of words in every document
ub_idf <- log10(ncol(ub_tdm)/apply(ub_tdm, 1, nnzero))
ub_tfidf <- ub_tf*ub_idf       #TF-IDF
ub_tfidf <- apply(ub_tfidf, 1, sum)
View(head(ub_tfidf[order(ub_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
b_tf <- as.matrix(b_tdm)/apply(b_tdm, 2, sum)   #term frequency: the number of words in every document
b_seg <- Corpus(VectorSource(b_seg))
b_tdm <- TermDocumentMatrix(b_seg, control = list(wordLengths = c(1,10)))
b_tf <- as.matrix(b_tdm)/apply(b_tdm, 2, sum)   #term frequency: the number of words in every document
b_idf <- log10(ncol(b_tdm)/apply(b_tdm, 1, nnzero))
b_tfidf <- b_tf*b_idf       #TF-IDF
b_tfidf <- apply(b_tfidf, 1, sum)
View(head(b_tfidf[order(b_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ug_seg <- Corpus(VectorSource(ug_seg))
ug_tdm <- TermDocumentMatrix(ug_seg, control = list(wordLengths = c(1,10)))
ug_tf <- as.matrix(ug_tdm)/apply(ug_tdm, 2, sum)   #term frequency: the number of words in every document
ug_idf <- log10(ncol(ug_tdm)/apply(ug_tdm, 1, nnzero))
ug_tfidf <- ug_tf*ug_idf       #TF-IDF
ug_tfidf <- apply(ug_tfidf, 1, sum)
View(head(ug_tfidf[order(ug_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
ub_seg <- Corpus(VectorSource(ub_seg))
ub_tdm <- TermDocumentMatrix(ub_seg, control = list(wordLengths = c(1,10)))
ub_tf <- as.matrix(ub_tdm)/apply(ub_tdm, 2, sum)   #term frequency: the number of words in every document
ub_idf <- log10(ncol(ub_tdm)/apply(ub_tdm, 1, nnzero))
ub_tfidf <- ub_tf*ub_idf       #TF-IDF
ub_tfidf <- apply(ub_tfidf, 1, sum)
View(head(ub_tfidf[order(ub_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
library(httr)
pageno <- 1:10    #?œå??æ•¸
g_url <- paste0("https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E5%A5%B3%E8%A3%9D&page=", pageno, "&sort=rnk/dc")
b_url <- paste0("https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E7%94%B7%E8%A3%9D&page=", pageno, "&sort=rnk/dc")
#url = "https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=%E5%A5%B3&page=1&sort=rnk/dc"
g_prods = character()
b_prods = character()
getcontent <- function(url, prods){
for(i in 1:length(pageno)){
res = GET(url[i])
res_json = httr::content(res)
results <- data.frame(do.call(rbind,res_json$prods))
prods[((pageno[i]-1)*20+1):(pageno[i]*20)] <- unlist(results$name)
}
return(prods)
}
g_prods <- c(g_prods, getcontent(g_url, g_prods))
Sys.sleep(60)
View(head(g_tfidf[order(g_tfidf, decreasing = TRUE)], n = 15L))       #TF-IDF
# Import library
library(e1071)
# Importing the dataset
data(iris)
# Create x and y
x <- subset(iris, select = -Species)
q()
getwd()
setwd("D:\D disk\unicourse\106shia\github\1062CSX_project\week_11\course_11")
setwd("D:/D disk/unicourse/106shia/github/1062CSX_project/week_11/course_11")
getwd()
#Clear the workspace
rm(list = ls())
#Loading the required packages
require(monmlp)
#Loading the required packages
install.packages(monmlp)
#Loading the required packages
install.packages("monmlp")
install.packages("Metrics")
#Loading the required packages
require(monmlp)
require(Metrics)
?logistic
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
y <- logistic(x) + rnorm(100, sd = 0.2)
logistic(x)
plot(logistic(x))
plot(x)
plot(y)
#monmlp::logistic
#Computes the logistic sigmoid function.
#Used as a hidden layer transfer function for nonlinear MLP or MONMLP models.
#
#Plotting Data
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
? monmlp.fit
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
mlpModel <- monmlp.predict(x = x, weights = mlpModel)
#Plotting predicted value over actual values
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
View(mlpModel)
attr(mlpModel, "ensemble")
attr(mlpModel, "ensemble")[[i]]
attr(mlpModel, "ensemble")[[2]]
mlpModel[[1]]
?attr
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
View(mlpModel)
mlpModel[[1]][["W1"]]
mlpModel <- monmlp.predict(x = x, weights = mlpModel)
attr(mlpModel, "ensemble")
View( attr(mlpModel, "ensemble"))
View( attr(mlpModel))
?monmlp.fit
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 5, bag = TRUE)
mlpModel <- monmlp.predict(x = x, weights = mlpModel)
#Plotting predicted value over actual values
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 10, bag = TRUE)
mlpModel <- monmlp.predict(x = x, weights = mlpModel)
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "blue")
}
cat ("MSE for Gradient Descent Trained Model: ", mse(y, mlpModel))
mlpModel
?mse
mse(y, mlpModel)
mse(y, attr(mlpModel))
mse(y, attr(mlpModel,"ensemble")[[1]])
mse(y, attr(mlpModel,"ensemble")[[2]])
?vapply()
vapply(attr(mlpModel,"ensemble"),mse,y)
lapply(attr(mlpModel,"ensemble"),mse)
lapply(attr(mlpModel,"ensemble"),mse(y,.))
mse(y, attr(mlpModel,"ensemble")[[i]])
for(i in 1:15){
mse(y, attr(mlpModel,"ensemble")[[i]])
}
for(i in 1:10){
mse(y, attr(mlpModel,"ensemble")[[i]])
}
for(i in 1:10){
m<-mse(y, attr(mlpModel,"ensemble")[[i]])
}
for(i in 1:10){
m[i]<-mse(y, attr(mlpModel,"ensemble")[[i]])
}
mse(y, mlpModel)
mean(m)
m
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
lines(x,mlpModel,col = "blue",lwd=4)
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
lines(x, mlpModel, lwd = 4, col = "blue")
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
lines(x, mlpModel, lwd = 4, col = "blue")
install.packages("RSNNS")
####################################################################
#Conjugate Gradient Trained NN
#install.packages('RSNSS')
require(RSNNS)
conGradMLP <- mlp(x = x, y = y,
size = (2/3)*nrow(x)*2,
maxit = 200,
learnFunc = "SCG")
y_h <- predict(conGradMLP, x)
cat ("MSE for Conjugate Gradient Descent Trained Model: ", mse(y_h, y))
?mlp
install.packages("neuralnet")
#æ•´ç†è³‡æ–™
data <- iris
data$setosa <- ifelse(data$Species == "setosa", 1, 0)
data$versicolor <- ifelse(data$Species == "versicolor", 1, 0)
data$virginica <- ifelse(data$Species == "virginica", 1, 0)
#è¨“ç·´æ¨¡åž‹
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
View(data)
#æ•´ç†è³‡æ–™
data <- iris
View(data)
data$setosa <- ifelse(data$Species == "setosa", 1, 0)
data$versicolor <- ifelse(data$Species == "versicolor", 1, 0)
data$virginica <- ifelse(data$Species == "virginica", 1, 0)
?ifelse
# Conditional Element Selection
# ifelse(test, yes, no)
# ifelse returns a value with the same shape as test which is filled with elements selected from either yes or no
# depending on whether the element of test is TRUE or FALSE.
#è¨“ç·´æ¨¡åž‹
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
f1
class(f1)
mode(f1)
?as.formula
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
#è¼‰å…¥å¥—ä»¶
library(neuralnet)
# Conditional Element Selection
# ifelse(test, yes, no)
# ifelse returns a value with the same shape as test which is filled with elements selected from either yes or no
# depending on whether the element of test is TRUE or FALSE.
#è¨“ç·´æ¨¡åž‹
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
print(bpn)
#åœ–è§£BP
plot(bpn)
plot.nn(bpn)
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01,linear.output = F)
#åœ–è§£BP
plot(bpn)
#åœ–è§£BP
plot(bpn)
library(devtools)
library(DMwR)
library(nnet)
library(reshape)
library(scales)
library(ggplot2)
install.packages("DMwR")
install.packages("reshape")
library(DMwR)
library(reshape)
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
?source_url
#ç¯„ä¾‹ä½¿ç”¨irisdata
data(iris)
?iris
?data
#(2)åˆ†ç‚ºè¨“ç·´çµ„å’Œæ¸¬è©¦çµ„è³‡æ–™é›†
set.seed(1117)
#å–å¾—ç¸½ç­†æ•¸
n <- nrow(iris)
nrow(iris)
round(0.7 * n)
0.7 * n
#è¨­å®šè¨“ç·´æ¨£æœ¬æ•¸70%
t_size = round(0.7 * n)
seq_len(n)
n
?seq_len
#å–å‡ºæ¨£æœ¬æ•¸çš„idx
t_idx <- sample(seq_len(n), size = t_size)
#è¨“ç·´çµ„æ¨£æœ¬
traindata <- iris[t_idx,]
head(traindata)
- t_idx
#æ¸¬è©¦çµ„æ¨£æœ¬
testdata <- iris[ - t_idx,]
head(traindata)
head(testdata)
tail(traindata)
?nnet
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
#(3)ç•«åœ–
plot.nnet(nnetM, wts.only = F)
#(4)é æ¸¬
#testçµ„åŸ·è¡Œé æ¸¬
prediction <- predict(nnetM, testdata, type = 'class')
#é æ¸¬çµæžœ
cm <- table(x = testdata$Species, y = prediction, dnn = c("å¯¦éš›", "é æ¸¬"))
cm
#An Introduction To Deep Learning
#Chapter 4 Code Examples
#Clear the workspace
rm(list = ls())
#Loading the required packages
#install.packages(c('nnet', 'pROC'))
require(nnet)
require(pROC)
install.packages("pROC")
#Summary Statistics Function
#We will use this later to evaluate our model performance
summaryStatistics <- function(array){
Mean <- mean(array)
Std <- sd(array)
Min <- min(array)
Max <- max(array)
Range <- Max - Min
output <- data.frame("Mean" = Mean, "Std Dev" = Std, "Min" =  Min,"Max" = Max, "Range" = Range)
return(output)
}
#Load Data
#Modifying Data From Iris Data Set
#Upload the necessary data
data  <- read.csv("./SpeedDating.csv", header = TRUE, stringsAsFactors = TRUE)
View(data)
str(data)
summary(data)
#Data Preprocessing
#Creating Repsponse Variable
second_date  <- ifelse(data[,1] + data[,2] == 2, 1, 0)
data  <- cbind(second_date,data)
#Transforming Charcter Vectors into Numerical vectors for feature selection
data$RaceM <- as.factor(data$RaceM)
data$RaceF <- as.factor(data$RaceF)
str(data)
data$RaceM <- as.numeric(data$RaceM)
data$RaceF <- as.numeric(data$RaceF)
str(data)
?complete.cases
complete.cases(data)
#Removing NA Values
data <- data[complete.cases(data), ]
?prcomp
prcomp(data, scale = TRUE)
#Performing Variable Selection
pca_data <- prcomp(data, scale = TRUE)
View(pca_data)
pca_data[["x"]]
summary(pca_data)
summary(pca_data)$importance
summary(pca_data)$importance[2,]
which(stdev_data >= .04)
stdev_data <- summary(pca_data)$importance[2,]
which(stdev_data >= .04)
data <- data[, which(stdev_data >= .04)]
singleLayerPerceptron <- function(max_iter = 3000, tol = .001){
#Initializing weights and other parameters
x_train <- data[, 2:ncol(data)]
y_train <- data[,1]
weights <- matrix(rnorm(ncol(x_train)))
cost <- 0
iter <- 1
converged <- FALSE
AUC <- c()
while(converged == FALSE){
#Cross Validating Data
rows <- sample(1:200, 200, replace = FALSE)
x_train <- as.matrix(x_train[rows, 1:ncol(x_train)])
y_train <- y_train[rows]
#Single Layer Perceptron
#Our Log Odds Threshold hear is the Average Log Odds
weighted_sum <- 1/(1 + exp(-(x_train%*%weights)))
y_h <- ifelse(weighted_sum <= mean(weighted_sum), 1, 0)
error <- 1 - roc(as.factor(y_h), y_train)$auc
AUC <- append(AUC, roc(as.factor(y_h), y_train)$auc)
#Weight Updates using Gradient Descent
#Error Statistic := 1 - AUC
if (abs(cost - error) > tol || iter < max_iter){
cost <- error
iter <-  iter + 1
gradient <- matrix(ncol = ncol(weights), nrow = nrow(weights))
for(i in 1:nrow(gradient)){
gradient[i,1] <- (1/length(y_h))*(0.01*error)*(weights[i,1])
}
#Updating weights based on gradient with respect to each weight
for (i in 1:nrow(weights)){
weights[i,1] <- weights[i,1] - gradient[i,1]
}
} else {
converged <- TRUE
}
}
#Performance Statistics
cat("The AUC of the Trained Model is ", roc(as.factor(y_h), y_train)$auc)
cat("\nTotal number of iterations: ", iter)
curve <- roc(as.factor(y_h), y_train)
plot(curve, main = "ROC Curve for Single Layer Perceptron")
cat("\nSummary Statistics of AUC over", iter, "iterations\n")
summaryStatistics(AUC)
}
singleLayerPerceptron()
#install.packages(c('nnet', 'pROC'))
require(nnet)
require(pROC)
singleLayerPerceptron <- function(max_iter = 3000, tol = .001){
#Initializing weights and other parameters
x_train <- data[, 2:ncol(data)]
y_train <- data[,1]
weights <- matrix(rnorm(ncol(x_train)))
cost <- 0
iter <- 1
converged <- FALSE
AUC <- c()
while(converged == FALSE){
#Cross Validating Data
rows <- sample(1:200, 200, replace = FALSE)
x_train <- as.matrix(x_train[rows, 1:ncol(x_train)])
y_train <- y_train[rows]
#Single Layer Perceptron
#Our Log Odds Threshold hear is the Average Log Odds
weighted_sum <- 1/(1 + exp(-(x_train%*%weights)))
y_h <- ifelse(weighted_sum <= mean(weighted_sum), 1, 0)
error <- 1 - roc(as.factor(y_h), y_train)$auc
AUC <- append(AUC, roc(as.factor(y_h), y_train)$auc)
#Weight Updates using Gradient Descent
#Error Statistic := 1 - AUC
if (abs(cost - error) > tol || iter < max_iter){
cost <- error
iter <-  iter + 1
gradient <- matrix(ncol = ncol(weights), nrow = nrow(weights))
for(i in 1:nrow(gradient)){
gradient[i,1] <- (1/length(y_h))*(0.01*error)*(weights[i,1])
}
#Updating weights based on gradient with respect to each weight
for (i in 1:nrow(weights)){
weights[i,1] <- weights[i,1] - gradient[i,1]
}
} else {
converged <- TRUE
}
}
#Performance Statistics
cat("The AUC of the Trained Model is ", roc(as.factor(y_h), y_train)$auc)
cat("\nTotal number of iterations: ", iter)
curve <- roc(as.factor(y_h), y_train)
plot(curve, main = "ROC Curve for Single Layer Perceptron")
cat("\nSummary Statistics of AUC over", iter, "iterations\n")
summaryStatistics(AUC)
}
singleLayerPerceptron()
data
View(data)
data[,1]
weights
matrix(rnorm(ncol(x_train)))
#Initializing weights and other parameters
x_train <- data[, 2:ncol(data)]
y_train <- data[,1]    #Decision M&F éƒ½ç‚º1->(1)ï¼Œæˆ–å…¶ä¸­ä¸€å€‹ä¸ç‚º1->(0)ã€‚
matrix(rnorm(ncol(x_train)))
ncol(x_train)
cost <- 0
iter <- 1
converged <- FALSE
AUC <- c()
#Cross Validating Data   #æ¯å€‹ç‰¹å¾µçš„è³‡æ–™å¤§å°ç‚º200
rows <- sample(1:200, 200, replace = FALSE)
rows
[rows, 1:ncol(x_train)]
x_train[rows, 1:ncol(x_train)]
install.packages("keras")
library(keras)
install_keras()
library(keras)
mnist <- dataset_mnist()
