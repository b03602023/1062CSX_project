freqFrame = as.data.frame(table(unlist(seg)))
# 觀察出現由多到寡的文字
View(freqFrame[order(freqFrame$Freq, decreasing = TRUE),])
windowsFonts(TC=windowsFont("Heiti TC Light"))
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:7)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
#適時的增加詞庫
# segment <- c("陳菊","布里斯本","高雄","重劃區","合作會","後勁溪")
segment <- c("低碳生活部落格","綠建築","節能","氣候變遷","電動車","台達電子")
new_user_word(mixseg,segment)   #Add user word
#有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
#轉成文件
freqFrame = as.data.frame(table(unlist(seg)))
windowsFonts(TC=windowsFont("Heiti TC Light"))
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),
min.freq=5,max.words=50,
random.order=FALSE,random.color=FALSE,
rot.per=.2, colors=brewer.pal(11, "Paired")[c(1:7)],
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE,family="TC")
# 觀察出現由多到寡的文字
View(freqFrame[order(freqFrame$Freq, decreasing = TRUE),])
#適時的增加詞庫
segment <- c("低碳環境部落客","綠建築","節能","氣候","台達電子","全球暖化")
new_user_word(mixseg,segment)   #Add user word
#適時的增加詞庫
# segment <- c("陳菊","布里斯本","高雄","重劃區","合作會","後勁溪")
segment <- c("低碳生活部落格","綠建築","節能","氣候變遷","電動車","台達電子")
new_user_word(mixseg,segment)   #Add user word
#有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
#轉成文件
freqFrame = as.data.frame(table(unlist(seg)))
# 觀察出現由多到寡的文字
View(freqFrame[order(freqFrame$Freq, decreasing = TRUE),])
#畫出文字雲
library(extrafont)
#適時的增加詞庫
# segment <- c("陳菊","布里斯本","高雄","重劃區","合作會","後勁溪")
segment <- c("低碳生活部落格","綠建築","節能","氣候變遷","電動車","台達電子","減碳")
new_user_word(mixseg,segment)   #Add user word
#有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
#轉成文件
freqFrame = as.data.frame(table(unlist(seg)))
# 觀察出現由多到寡的文字
View(freqFrame[order(freqFrame$Freq, decreasing = TRUE),])
# 觀察出現由多到寡的文字
View(freqFrame[order(freqFrame$Freq, decreasing = TRUE),])
# 觀察出現由多到寡的文字
View(freqFrame[order(freqFrame$Freq, decreasing = TRUE),])
View(freqFrame
)]
View(freqFrame
)
freqFrame[1,]
freqFrame$Var1
freqFrame$Var1=="雙城記"
freqFrame[freqFrame$Var1=="雙城記",]
install.packages("Rfacebook")
#---------Rfacebook--------
token = "EAACEdEose0cBAPWqw5JDTSaYbWgWYcCsGJ11ZAuZBbFR6ZAZBlahtxdas6Sk2ERLKRBW1V3rZCJNjXo1Fp26ABZA85gyiLgi85fDhUt4CbwTIZBcLTUmsZCZCJNUwKvpCW2o5m2DxSWvby8TS5v0ETvk2srZB7UvU43TvcVPlR6pcz4CaFMvLYQxnZB92efbGzrZAX8ZD"
me <- getUsers("me", token, private_info = TRUE)
library(Rfacebook)
me <- getUsers("me", token, private_info = TRUE)
me$name
require("Rfacebook")
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
token = "EAACEdEose0cBAPWqw5JDTSaYbWgWYcCsGJ11ZAuZBbFR6ZAZBlahtxdas6Sk2ERLKRBW1V3rZCJNjXo1Fp26ABZA85gyiLgi85fDhUt4CbwTIZBcLTUmsZCZCJNUwKvpCW2o5m2DxSWvby8TS5v0ETvk2srZB7UvU43TvcVPlR6pcz4CaFMvLYQxnZB92efbGzrZAX8ZD"
library(Rfacebook)
me <- getUsers("me", token, private_info = TRUE)
me$name
fb.oauth <- fbOAuth(
app_id="221670988578368",
app_secret="b90cd74f3f117ed3614cf2836b914016",
extended_permissions = TRUE)
#---------Rfacebook--------
token = "EAACEdEose0cBAPWqw5JDTSaYbWgWYcCsGJ11ZAuZBbFR6ZAZBlahtxdas6Sk2ERLKRBW1V3rZCJNjXo1Fp26ABZA85gyiLgi85fDhUt4CbwTIZBcLTUmsZCZCJNUwKvpCW2o5m2DxSWvby8TS5v0ETvk2srZB7UvU43TvcVPlR6pcz4CaFMvLYQxnZB92efbGzrZAX8ZD"
library(Rfacebook)
me <- getUsers("me", token, private_info = TRUE)
me$name
require("Rfacebook")
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
?fbOAuth
fbOAuth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
token = "EAACEdEose0cBAPWqw5JDTSaYbWgWYcCsGJ11ZAuZBbFR6ZAZBlahtxdas6Sk2ERLKRBW1V3rZCJNjXo1Fp26ABZA85gyiLgi85fDhUt4CbwTIZBcLTUmsZCZCJNUwKvpCW2o5m2DxSWvby8TS5v0ETvk2srZB7UvU43TvcVPlR6pcz4CaFMvLYQxnZB92efbGzrZAX8ZD"
library(Rfacebook)
me <- getUsers("me", token, private_info = TRUE)
me$name
require("Rfacebook")
fbOAuth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
fbOAuth
#---------Rfacebook--------
token = "EAACEdEose0cBAPWqw5JDTSaYbWgWYcCsGJ11ZAuZBbFR6ZAZBlahtxdas6Sk2ERLKRBW1V3rZCJNjXo1Fp26ABZA85gyiLgi85fDhUt4CbwTIZBcLTUmsZCZCJNUwKvpCW2o5m2DxSWvby8TS5v0ETvk2srZB7UvU43TvcVPlR6pcz4CaFMvLYQxnZB92efbGzrZAX8ZD"
library(Rfacebook)
me <- getUsers("me", token, private_info = TRUE)
me$name
require("Rfacebook")
fbOAuth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
me <- getUsers("me",token=fb.oauth)
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
me <- getUsers("me",token=fb.oauth)
me$name
token = "EAACEdEose0cBAPWqw5JDTSaYbWgWYcCsGJ11ZAuZBbFR6ZAZBlahtxdas6Sk2ERLKRBW1V3rZCJNjXo1Fp26ABZA85gyiLgi85fDhUt4CbwTIZBcLTUmsZCZCJNUwKvpCW2o5m2DxSWvby8TS5v0ETvk2srZB7UvU43TvcVPlR6pcz4CaFMvLYQxnZB92efbGzrZAX8ZD"
library(Rfacebook)
me <- getUsers("me", token, private_info = TRUE)
me$name
install.packages("httpuv")
install.packages("httpuv")
library(httpuv)
require("Rfacebook")
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE,
type = "application/x-www-form-urlencoded"
)
require("Rfacebook")
fb.oauth <- fbOAuth(
app_id="599990343682885",
app_secret="5162044ac94f64461ed587312d424521",
extended_permissions = TRUE)
setwd("D:/D disk/unicourse/106shia/github/1062CSX_project/week_5/project_5")
filenames <- list.files(getwd(), pattern="*.txt")  #pattern: an optional regular expression. Only file names which match the regular expression will be returned.
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)      #斷詞用
library(RColorBrewer)
library(wordcloud)
library(tmcn)   #segmentCN
filenames <- list.files(getwd(), pattern="*.txt")  #pattern: an optional regular expression. Only file names which match the regular expression will be returned.
files <- lapply(filenames, readLines)  #Read some or all text lines from a connection.
docs <- Corpus(VectorSource(files))  #Representing and computing on corpora(語料庫).
View(docs)
docs[["285"]][["content"]]
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
# 清除大小寫英文與數字
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#-------斷詞--------
mixseg = worker()
segment <- c("新北")
new_user_word(mixseg,segment)   #Add user word
#斷詞  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
View(docs)
docs[["285"]][["content"]]
View(seg)
seg[[285]]
freqFrame = as.data.frame(table(unlist(seg)))
View(files)
View(freqFrame)
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
View(seg)
View(tdm)
inspect(tdm)
seg = lapply(docs, jieba_tokenizer)
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
?TermDocumentMatrix
tdm <- TermDocumentMatrix(docs, control = list(wordLengths = c(1,10)))
inspect(tdm)
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
View(files)
#詞頻結果:
library(Matrix)   #nnzero
freqFrame = as.data.frame(table(unlist(seg)))
# View(freqFrame[order(freqFrame$Freq),])
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
#轉成文件
#詞頻結果:
library(Matrix)   #nnzero
freqFrame = as.data.frame(table(unlist(seg)))
# View(freqFrame[order(freqFrame$Freq),])
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
tfidf <- tf*idf       #TF-IDF
# View(head(tfidf))       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
all <- cbind(s_tf, idf, s_tfidf)
colnames(all) <- c("TF", "IDF", "TF-IDF")
s_tfidf <- s_tfidf[order(s_tfidf, decreasing = TRUE)]
#s_tfidf[s_tfidf>0.23]
tfidf <- as.data.frame(tfidf)
#------visualize----
library(ggplot2)
t <- 0.28
p <- ggplot(tfidf[s_tfidf>t,], aes(names(s_tfidf[s_tfidf>t]), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
)+
coord_flip()+
xlab("TF-IDF") + ylab("字詞")
p
View(s_tfidf[s_tfidf>0.23])
t <- 10
p <- ggplot(tfidf[s_tfidf>t,], aes(names(s_tfidf[s_tfidf>t]), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
)+
coord_flip()+
xlab("TF-IDF") + ylab("字詞")
(tfidf)
head(tfidf)
View(seg)
#斷詞  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
#轉成文件
freqFrame = as.data.frame(table(unlist(seg)))
# View(freqFrame[order(freqFrame$Freq),])
seg <- Corpus(VectorSource(seg))
View(seg)
View(seg)
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
inspect(tdm)
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
View(tf)
View(head(tf))
View(head(tf)[,1:20])
tfidf <- tf*idf       #TF-IDF
# View(head(tfidf))       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
all <- cbind(s_tf, idf, s_tfidf)
colnames(all) <- c("TF", "IDF", "TF-IDF")
s_tfidf <- s_tfidf[order(s_tfidf, decreasing = TRUE)]
# View(head(tfidf))       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
s_tfidf <- s_tfidf[order(s_tfidf, decreasing = TRUE)]
#s_tfidf[s_tfidf>0.23]
tfidf <- as.data.frame(tfidf)
#------visualize----
library(ggplot2)
t <- 10
p <- ggplot(tfidf[s_tfidf>t,], aes(names(s_tfidf[s_tfidf>t]), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
)+
coord_flip()+
xlab("字詞") + ylab("TF-IDF")
View(head(tfidf)[,1:20])
s_tfidf[s_tfidf>0.23]
View(s_tfidf[s_tfidf>0.23])
t <- 2.6
p <- ggplot(tfidf[s_tfidf>t,], aes(names(s_tfidf[s_tfidf>t]), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
)+
coord_flip()+
xlab("字詞") + ylab("TF-IDF")
segment <- read.csv("keyword.csv", header=FASLE)
segment <- read.csv("keyword.csv", header=FALSE)
View(segment)
new_user_word(mixseg,segment)   #Add user word
segment <- as.character(read.csv("keyword.csv", header=FALSE))
new_user_word(mixseg,segment)   #Add user word
segment
segment <- as.character(read.csv("keyword.csv", header=FALSE))
segment <- as.vector(read.csv("keyword.csv", header=FALSE))
View(segment)
new_user_word(mixseg,segment)   #Add user word
segment <- read.csv("keyword.csv", header=FALSE)
class(segment)
mode(segment)
filenames
mode(filenames)
class(filenames)
segment <- read.csv("keyword.csv", header=FALSE)
as.character(segment)
?as.character
is.character(segment)
segment
apply(segment, 1, paste0)
segment <- apply(segment, 1, paste0)
new_user_word(mixseg,segment)   #Add user word
#斷詞  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
keyword <- read.csv("keyword.csv", header=FALSE)
#-------斷詞--------
# 加入新字詞
mixseg = worker()
keyword <- read.csv("keyword.csv", header=FALSE)
keyword <- apply(keyword, 1, paste0)
new_user_word(mixseg, keyword)   #Add user word
#斷詞  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
#轉成文件
#詞頻結果:
library(Matrix)   #nnzero
freqFrame = as.data.frame(table(unlist(seg)))
# View(freqFrame[order(freqFrame$Freq),])
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
# View(head(tf)[,1:20])
idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
tfidf <- tf*idf       #TF-IDF
# View(head(tfidf)[,1:20])       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
s_tfidf <- s_tfidf[order(s_tfidf, decreasing = TRUE)]
#View(s_tfidf[s_tfidf>0.23])
tfidf <- as.data.frame(tfidf)
#------visualize----
library(ggplot2)
t <- 2.6
p <- ggplot(tfidf[s_tfidf>t,], aes(names(s_tfidf[s_tfidf>t]), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
)+
coord_flip()+
xlab("字詞") + ylab("TF-IDF")
#-------斷詞--------
# 加入新字詞
mixseg = worker()
keyword <- read.csv("keyword.csv", header=FALSE)
keyword <- apply(keyword, 1, paste0)
new_user_word(mixseg, keyword)   #Add user word
#斷詞  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
#轉成文件
#詞頻結果:
library(Matrix)   #nnzero
freqFrame = as.data.frame(table(unlist(seg)))
# View(freqFrame[order(freqFrame$Freq),])
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
# View(head(tf)[,1:20])
idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
tfidf <- tf*idf       #TF-IDF
# View(head(tfidf)[,1:20])       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
s_tfidf <- s_tfidf[order(s_tfidf, decreasing = TRUE)]
#View(s_tfidf[s_tfidf>0.23])
tfidf <- as.data.frame(tfidf)
#------visualize----
library(ggplot2)
t <- 2.6
p <- ggplot(tfidf[s_tfidf>t,], aes(names(s_tfidf[s_tfidf>t]), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
)+
coord_flip()+
xlab("字詞") + ylab("TF-IDF")
View(freqFrame[order(freqFrame$Freq),1:10])
View(freqFrame[order(freqFrame$Freq),])
View(head(tf)[,1:20])
View(head(tdm)[,1:20])
View(tdm)
# View(head(as.matrix(tdm))[,1:14])
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
View(head(as.matrix(tdm))[,1:14])
termfreq <- as.matrix(tdm)  #詞頻向量
View(termfreq)
#------cos similarity ranking-------
View(head(tfidf)[,1:10])
View(s_tfidf[s_tfidf>0.23])
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)      #斷詞用
library(RColorBrewer)
library(wordcloud)
library(tmcn)   #segmentCN
filenames <- list.files(getwd(), pattern="*.txt")  #pattern: an optional regular expression. Only file names which match the regular expression will be returned.
files <- lapply(filenames, readLines)  #Read some or all text lines from a connection.
docs <- Corpus(VectorSource(files))  #Representing and computing on corpora(語料庫).
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
# 清除大小寫英文與數字
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#-------斷詞--------
# 加入新字詞
mixseg = worker()
keyword <- read.csv("keyword.csv", header=FALSE)
keyword <- apply(keyword, 1, paste0)
new_user_word(mixseg, keyword)   #Add user word
#斷詞  mixseg[groups]
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
#轉成文件
#詞頻結果:
library(Matrix)   #nnzero
freqFrame = as.data.frame(table(unlist(seg)))
# View(freqFrame[order(freqFrame$Freq),])
seg <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(seg, control = list(wordLengths = c(1,10)))
#termfreq <- as.matrix(tdm)  #詞頻向量
# View(head(as.matrix(tdm))[,1:14])
tf <- as.matrix(tdm)/apply(tdm, 2, sum)   #term frequency: the number of words in every document
# View(head(tf)[,1:20])
idf <- log10(ncol(tdm)/apply(tdm, 1, nnzero))
tfidf <- tf*idf       #TF-IDF
# View(head(tfidf)[,1:20])       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
# View(head(tfidf)[,1:20])       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
s_tfidf <- s_tfidf[order(s_tfidf, decreasing = TRUE)]
View(s_tfidf[s_tfidf>0.23,1:10])
View(s_tfidf[s_tfidf>0.23][1:10])
t <- 30
p <- ggplot(tfidf[s_tfidf>t,], aes(names(s_tfidf[s_tfidf>t]), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
)+
coord_flip()+
xlab("字詞") + ylab("TF-IDF")
tfidf <- tf*idf       #TF-IDF
# View(head(tfidf)[,1:20])       #TF-IDF
s_tfidf <- apply(tfidf, 1, sum)
s_tfidf <- s_tfidf[order(s_tfidf, decreasing = TRUE)]
#View(s_tfidf[s_tfidf>0.23][1:10])
tfidf <- as.data.frame(tfidf)
#------cos similarity ranking-------
# View(head(tfidf)[,1:10])
#------visualize----
library(ggplot2)
t <- 30
p <- ggplot(tfidf[s_tfidf>t,], aes(names(s_tfidf[s_tfidf>t]), s_tfidf[s_tfidf>t]))
p + geom_bar(stat = "identity")+
theme(axis.text.y = element_text(angle = 60, hjust = 1)
)+
coord_flip()+
xlab("字詞") + ylab("TF-IDF")
